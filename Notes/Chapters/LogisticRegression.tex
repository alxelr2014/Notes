\chapter{Logistic Regression}
In machine learning we wish to optimize a function like \(\func{J}{\theta}\). Usually a function in form
\begin{align*}
    \func{J}{\theta} & = \left(\dfrac{1}{n} \sum_{i = 1}^n \func{L}{\func{h}{x^{(i)}; \theta} , y^{(i)}} \right) + \lambda \func{R}{\theta} \\
                     & = \CalE_n + \lambda \func{R}{\theta}
\end{align*}
where \(R\) is the \textbf{regularization function} and \(\lambda\) is a hyperparameter. A common regularizer is
\begin{equation*}
    \func{R}{\theta} = \norm[\theta - \theta_{\mathrm{prior}}]^2
\end{equation*}
where \(\theta_{\mathrm{prior}}\) is the value that we want \(\theta\) to be close to.

\section{Linear logistic classifier}
The problem of minimizing 0-1 Loss problem is NP-hard. A problem with sign is that incremental change are hard to find because of the discrete nature of the function hence, to smooth out the sign function we use \text{sigmoid} function
\begin{equation*}
    \func{\sigma}{x} = \dfrac{1}{1 + e^{-x}}
\end{equation*}
\tikzsetnextfilename{Chapters/graphics/sigmoid/sigmoid_function}
\begin{center}\begin{tikzpicture}
        \begin{axis}[
                axis lines = center,
                xlabel = \(x\),
                ylabel = {\(\func{\sigma}{x}\)},
                xmin = -5, xmax = 5,
                ymin = -0.5 , ymax = 1.5,
                xtick ={-5,0,5},
                ytick = {-0.5,0,0.5,1}
            ]
            \addplot [
                line width = 1.5,
                domain=-5:5,
                samples=100,
                color=black,
            ]
            {1/(1 + e^(-x))};
            \definecolor{grey}{rgb}{0.635, 0.674, 0.690}   
            \addplot [
                line width = 0.8,
                domain= -5:5,
                samples = 100,
                color = grey,
                dashed, 
                ]{1.0};
        \end{axis}
\end{tikzpicture}
\end{center}


Equivalently, we want to make classifier that predict \(+1\) when \(\func{\sigma}{\theta^T x+ \theta_0} > p = 0.5\) and \(-1\) otherwise. The value \(p\) is called the \textbf{prediction threshold}.

Loss on all data is inversely related to the probability that \((\theta , \theta_0)\) assigns to the data. Assuming that points in data set are independent.

\begin{align*}
    g^{(i)} & =  \func{\sigma}{\theta^T x + \theta_0} \\
    p^{(i)} & = \begin{cases}
        g^{(i)}     & \text{if} \quad y^{(i)} = 1 \\
        1 - g^{(i)} & \text{if} \quad y^{(i)} = 0 \\
    \end{cases}
\end{align*}
and we wish to maximize the probability (\(\Theta\) here means \((\theta, \theta_0)\)):
\begin{equation*}
    \func{p}{\Theta ; \CalD_n} = \prod_{i = 1}^n p^{(i)} = \prod_{i = 1}^n (g^{(i)})^{y^{(i)}} ( 1 - g^{(i)})^{(1 - y^{(i)})}                   \\
\end{equation*}

Using the log-likelihood
\begin{equation*}
    \implies \func{\CalL_{LL}}{p} = \sum_{i = 1}^n \func{\CalL_{LL}}{g^{(i)} , y^{(i)}} =\sum_{i = 1}^n  y^{(i)} \func{\ln}{g^{(i)}} + \left(1 - y^{(i)} \right) \func{\ln}{ 1 - g^{(i)}} 
\end{equation*}
then our loss function would be
\begin{equation*}
    \func{L}{\func{h}{x^{(i)}, \Theta} , y^{(i)}} = - \func{\CalL_{LL}}{g^{(i)} , y^{(i)}} =\func{\CalL_{NLL}}{g^{(i)} , y^{(i)}} 
\end{equation*}

hence we wish to minimize the 

\begin{equation*}
    \func{J}{\Theta}  = \left(\dfrac{1}{n} \sum_{i = 1}^n \func{\CalL_{NLL}}{ \func{\sigma}{\theta^T x^{(i)} + \theta_0} , y^{(i)}} \right) + \lambda \norm[\theta]^2
\end{equation*}

note that we didn't include \(\theta_0\) in the regularizer as we don't want to punish large \(\theta_0\). 

\section{Gradient descent}
\subsection{Batch gradient descent}
\begin{algorithm}[H]
    \DontPrintSemicolon
    $\theta^{(0)} = \theta_{\mathrm{init}} $\;
    $t = 0 $\;

    \Repeat{$\abs[\func{f}{\theta^{(t)}} - \func{f}{\theta^{(t - 1)}}] < \epsilon $}{
        $ \theta^{(t)} = \theta^{(t-1)} - \eta \func{\nabla f}{\theta^{(t-1)}} $\;
    }

    \Return{$\theta $}
    \caption{batch gradient descent $(f, \nabla f, \theta_{\mathrm{init}} , \eta , \epsilon )$}
\end{algorithm}

\begin{theorem}
    If \(f\) is convex, for any desired accuracy \(\epsilon\) there is some \(\eta\) such that batch gradient descent will converge to \(\theta\) within \(\epsilon\) of the optimum.
\end{theorem}

Lets derive the closed form of gradient descent for the logistic regression \(J\) function. 
\begin{equation*}
    \nabla \func{J}{\Theta} = \left(\dfrac{1}{n} \sum_{i = 1}^n \nabla \func{\CalL_{NLL}}{ \func{\sigma}{\theta^T x^{(i)} + \theta_0} , y^{(i)}} \right) + \lambda \nabla \norm[\theta]^2
\end{equation*}
we have that 
\begin{align*}
    \PDiff{\func{\CalL_{NLL}}{g^{(i)} , y^{(i)}}}{\theta_j} &= - \dfrac{y^{(i)}}{g^{(i)}} \PDiff{g^{i}}{\theta_j} + \dfrac{1- y^{(i)}}{1 - g^{(i)}} \PDiff{g^{i}}{\theta_j}\\
    &= \dfrac{g^{(i)} - y^{(i)}}{g^{i} \left( 1 - g^{(i)} \right)} \ODiff{\sigma(z)}{z} \PDiff{z}{\theta_j} \\
    \intertext{for simplicity assume \(x^{(i)}_0 = 1\)}
    &= \dfrac{g^{(i)} - y^{(i)}}{g^{i} \left( 1 - g^{(i)} \right)} \left(g^{(i)} - \left(g^{(i)}\right)^2 \right) x^{(i)}_j\\
    &= \left(g^{(i)} - y^{(i)}\right) x^{(i)}_j \\
    \implies \nabla \func{\CalL_{NLL}}{g^{(i)} , y^{(i)}} &=\left(g^{(i)} - y^{(i)}\right) x^{(i)} \\
    \intertext{also for the regularizer}
    \PDiff{\norm[\theta]^2}{\theta_j} &= \begin{cases}
        2 \theta_j & j \neq 0\\
        0 & j = 0
    \end{cases}
\end{align*}

Therefore, 

\begin{align*}
    \nabla_{\theta} \func{J}{\Theta} &= \left(\dfrac{1}{n} \sum_{i = 1}^n \left(g^{(i)} - y^{(i)}\right) x^{(i)} \right) + 2\lambda \theta\\
    \nabla_{\theta_0} \func{J}{\Theta} &= \dfrac{1}{n} \sum_{i = 1}^n g^{(i)} - y^{(i)} 
\end{align*}

\subsection{Stochastic gradient descent}
Suppose we want to minimize \(\func{f}{\Theta}\) which can be written as a sum of some \(\func{f_i}{\Theta}\)
\begin{equation*}
    \func{f}{\Theta} = \sum_{i = 1}^n \func{f_i}{\Theta}
\end{equation*}
Instead of adding up the gradient for each part every iteration, we can choose one of the functions and apply the gradient descent for that function only. We expect that running this algorithm for long enough gives us the optimum solution just like the gradient descent.

\begin{algorithm}[H]
    \DontPrintSemicolon
    $\Theta^{(0)} = \Theta_{\mathrm{init}} $\;
    \For{$t = 1 \to T $}{
        $i = $Random$(\set{1, \dots , n})$\; 
        $ \Theta^{(t)} = \Theta^{(t-1)} - \func{\eta}{t} \func{\nabla f_i}{\theta^{(t-1)}} $\;
    }

    \Return{$\theta $}
    \caption{stochastic gradient descent $(f, \nabla f_1 , \dots,  \nabla f_n, \Theta_{\mathrm{init}} , \eta , T )$}
\end{algorithm}

\begin{theorem}
    If \(f\) is convex and \(\func{\eta}{t}\) is sequence satisfying 
    \begin{equation*}
        \sum_{i= 1}^n \func{\eta}{t} = \infty \quad \mathrm{and} \quad \sum_{i= 1}^n \func{\eta^2}{t} < \infty
    \end{equation*}
    Then SGD converges almost sure to the optimal \(\Theta\).
\end{theorem}

\section{Linear regression}
In regression our output is a real number as oppose to a discrete value. Typically, we use \textit{squared error} for loss function.
\begin{equation*}
    \func{L}{g^{(i)}, y^{(i)}} = \left(g^{(i)} - y^{(i)}\right)^2
\end{equation*}

and hence our \(J\) function will be the \textit{mean squared error} 

\begin{equation*}
    \func{J}{\Theta} = \dfrac{1}{n} \sum_{i = 1}^n \left( g^{(i)} - y^{(i)} \right)^2 + \lambda \func{R}{\Theta}
\end{equation*}

\subsection{Linear hypothesis}
Discarding \(\theta_0\) (for simplicity) and the regularizer term and by allowing our hypothesis \(h\) to be linear --- that is \(g^{(i)} = \func{h}{x^{(i)}; \theta} = \theta^T x\)---, we can easily arrive at a closed form optimal solution. To do this consider the following definitions 
\begin{equation*}
    X = \begin{bmatrix}
        x^{(1)}_1 & \dots & x^{(1)}_d \\
        \vdots & \ddots & \vdots\\
        x^{(n)}_1 & \dots & x^{(n)}_d
    \end{bmatrix} \qquad Y = \begin{bmatrix}
        y^{(1)}\\
        \vdots \\
        y^{(n)} 
    \end{bmatrix}
\end{equation*}

then we can write \(\func{J}{\theta}\) as

\begin{equation*}
    \func{J}{\theta} = \frac{1}{n} (X\theta - Y)^T (X\theta-Y)
\end{equation*}
taking the gradient gives 
\begin{align*}
    \nabla \func{J}{\hat{\theta}} &= \dfrac{2}{n} X^T (X\hat{\theta} - Y) = 0\\
    \implies  X^T X\hat{\theta} &= X^T Y \implies \hat{\theta} = \left(X^T X\right)^{-1} X^T Y
\end{align*}
Assuming the invertibility of \(X^TX\), this minimizes the \(J\) function. However, we will run into the problem of \textit{overfitting}. By adding the regularizer back we can solve the problem of invertibility and overfitting at the same time. Consider the following \(J_{\mathrm{ridge}}\) function, 
\begin{align*}
    \func{J_{\mathrm{ridge}}}{\theta} &= \frac{1}{n} (X\theta - Y)^T (X\theta-Y) + \lambda \theta^T \theta \\
     \implies \nabla \func{J_{\mathrm{ridge}}}{\hat{\theta}} &= \dfrac{2}{n} X^T (X\hat{\theta} - Y) + 2\lambda \hat{\theta}  = 0\\ 
     \implies  (X^TX + n \lambda I) \hat{\theta} &= X^T Y \implies \hat{\theta}  = (X^TX + n \lambda I )^{-1} X^T Y
\end{align*}

A faster way of computing the optimal solution is using the SGD or BGD, which since the \(J_{\mathrm{ridge}}\) is convex will converge.
\subsection{Non-linear hypothesis}
A simple generalization to the non-linear case can be achieved by transforming the data using basis functions, \(\phi_i : \Reals^d \to \Reals\)  
\begin{equation*}
    \func{h}{x; \theta } = \theta_0 + \sum_{i = 1}^m \theta_i \func{\phi_i}{x}
\end{equation*}

Some examples of basis function include 
\begin{description}
    \item[Polynomial (univariate)] \(\func{\phi_i}{x} = x^i\) for \(i = 1, \dots, m\)
    \begin{equation*}
        \func{h}{x; \theta } =  \sum_{i = 0}^m \theta_i x^i
    \end{equation*}
    \item[Gaussian] preddiction based on similarity to \textit{prototypes}, \(\mu_1 , \dots , \mu_m\), where \(\sigma_i^2\) control how quickly it vanished as a function of the distance to the prototype.
    \begin{equation*}
        \func{\phi_i}{x} = \func{\exp}{-\dfrac{(x - \mu_i)^2}{2 \sigma_i^2}}
    \end{equation*} 
    \item[Sigmoid]
    \begin{equation*}
        \func{\phi_i}{x} = \func{\sigma}{\dfrac{\norm[x - \mu_i]}{\sigma_i}}
    \end{equation*} 
    \item[Fourier basis]
\end{description}

Deriving the optimal solution is like that case of linear hypothesis. Lets define the following matrices 
\begin{equation*}
    \Phi = \begin{bmatrix}
        1 & \func{\phi_1}{x^{(1)}} & \dots & \func{\phi_m}{x^{(1)}} \\
        1 & \func{\phi_1}{x^{(1)}}  & \dots & \func{\phi_m}{x^{(2)}} \\
        \vdots& \vdots & \ddots & \vdots\\
        1 & \func{\phi_1}{x^{(n)}}  & \dots & \func{\phi_m}{x^{(n)}} \\

    \end{bmatrix} \qquad Y = \begin{bmatrix}
        y^{(1)}\\
        \vdots \\
        y^{(n)} 
    \end{bmatrix} 
\end{equation*}
Then our objective \(\func{J}{\theta}\) can be written as 
\begin{align*}
    \func{J_{\mathrm{ridge}}}{\theta} &= \frac{1}{n} (\Phi\theta - Y)^T (\Phi\theta-Y) + \lambda \theta^T \theta \\
     \implies \nabla \func{J_{\mathrm{ridge}}}{\hat{\theta}} &= \dfrac{2}{n} \Phi^T (\Phi\hat{\theta} - Y) + 2\lambda \hat{\theta}  = 0\\ 
     \implies  (\Phi^T\Phi + n \lambda I) \hat{\theta} &= \Phi^T Y \implies \hat{\theta}  = (\Phi^T\Phi + n \lambda I )^{-1} \Phi^T Y
\end{align*}
\section{Regularizers}
\subsection{Overfitting}
Overfitting occurs when our hypothesis is tuned to the training data but it fails to generalize. Two main causes of overfitting are \textit{model complexity} and low number of training data. Therefore, one way to solve overfitting is to increase training data size. Some other systematic solutions to the overfitting problem inclue 
\begin{itemize}
    \item Selection of suitable model complexity and good parameters. One way to  evaluate a good model is \textit{hold-out} method, where we divide data into training and validation sets then we train a set of models and evaluated their performance on the validation set. However, this method is very wasteful of data and a small validation set obtains relatively noisy estimate of performance. A better method is the previously discussed cross validation.
    \item Adding a regularizer term. As discussed, a regularizer encourages the parameter to closer to some special value \(\theta_{\mathrm{prior}}\). When increase \(\lambda\) we tend to increase structural error but decrease estimation error and vice versa.
\end{itemize}
\subsection{Errors}
There are two kinds of way that a hypothesis might contribute to the error on test data.
\begin{description}
    \item [Structural error] There is no hypothesis in our class that performs well with the training data.
    \begin{equation*}
        \min \expected{(y - w^Tx)^2}
    \end{equation*}
    \item [Estimation error] The training data was not a good representaion of the test data and hence our hypothesis is not as good.
    \begin{align*}
        \min &\; \expected{(w^{\ast T} x - \hat{w}^T x)^2}\\
        w^\ast &= \argmin \expected{(y - w^Tx)^2}\\
        \hat{w} &= \argmin \sum_{i= 1}^{n} (y^{(i)} - w^T x^{(i)})^2 
    \end{align*}
    \item [Expected error] is the sum of the structural error and estimation error.
\end{description}
\subsection{Bias-variance trade-off}
need more, chapter 1.1 , 1.3 , 3.1, 3.2 and chapter 2.3, 3.2, 3.4