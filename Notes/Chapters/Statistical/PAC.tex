
\chapter{Probably Approximately Correct Learning}

\begin{definition}
    A hypothesis class \(\calH\) is PAC learnable if there exist a function \(m_{\calH} : \opop{0}{1}^2 \to \Naturals\) and a learning alogrithm such that:
    \begin{itemize}
        \item For every \(\epsilon,\delta \in \opop{0}{1}\), distribution \(\calD\) over \(\calX\), and labeling function \(f: \calX \to \set{0,1}\)
        \item If the realizable assumption hold with respect to \(\calH,\calD,f\)
        \item Then, when running the algorithm on \(m \geq \func{m_{\calH}}{\epsilon,\delta}\) of i.i.d. samples generated by \(\calD\) and labeled by \(f\), the algorithm returns a hypothesis \(h\) such that 
        \begin{equation*}
            \prob{\func{L_{\calD, f}}{h} \leq \epsilon} \geq 1- \delta
        \end{equation*}
    \end{itemize}
\end{definition}
\begin{remark}
    The minimal function \(m_{\calH}\) determines the sample complexity of learning \(\calH\).
\end{remark}

\begin{corollary}
    Every finite hypothesis class is PAC learnable with sample complexity 
    \begin{equation*}
        \func{m_{\calH}}{\epsilon, \delta} \leq \ceil{\frac{\func{\log}{\abs{\calH}/\delta }}{\epsilon}}
    \end{equation*}
\end{corollary}

Let \(\calJ\) be the joint distribution over \(\calX \times \calY\). Note that, \(\calD\) is the marginal distribution of \(\calJ\). Then we can revise the definition for the true error 
\begin{equation*}
    \func{L_{\calJ}}{h} = \func{\Probability_{(x,y) \sim \calJ}}{\func{h}{x} \neq y}
\end{equation*}
Then given \(\calJ\) the best label prediction function is 
\begin{equation*}
    \func{f_{\calJ}}{x} = \begin{cases}
        1 & \text{if} \ \condProb{y = 1}{x} \geq \frac{1}{2}\\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}
that is, there is no other classifier \(g\) with \(\func{L_{\calJ}}{g} < \func{L_{\calJ}}{f_{\calJ}}\)

\begin{definition}
    A hypothesis \(\calH\) is \textbf{agnostic PAC learnable} if there exist a function \(m_{\calH} : \opop{0}{1}^2 \to \Naturals\) and a learning alogrithm such that:
    \begin{itemize}
        \item For every \(\epsilon,\delta \in \opop{0}{1}\), distribution \(\calJ\) over \(\calX \times \calY\)
        \item Then, when running the algorithm on \(m \geq \func{m_{\calH}}{\epsilon,\delta}\) of i.i.d. samples generated by \(\calD\) and labeled by \(f\), the algorithm returns a hypothesis \(h\) such that 
        \begin{equation*}
            \prob{\func{L_{\calJ}}{h} \leq \min_{h'} \func{L_{\calJ}}{h'} + \epsilon} \geq 1- \delta
        \end{equation*}
    \end{itemize}
\end{definition}

\section{Generalized loss functions}
Given any set \(\calH\) and some domain \(Z\), let \(l\) be any function from \(\calH \times Z\) to \(\Reals_+\). We call such functions \textit{loss functions}. We then define the risk function to be
\begin{equation*}
    \func{L_{\calZ}}{h} = \expected[\calZ]{\func{l}{h,z}}
\end{equation*}
where \(h \in \calH\), and \(\calZ\) is the distribution on \(Z\). Similarly, the empirical risk over a given sample \(S \in Z^m\) is 
\begin{equation*}
    \func{L_S}{h} = \dfrac{1}{m} \sum_{i = 1}^m \func{l}{h,z_i}
\end{equation*}
Then revising the agnostic PAC learnability definition for general loss function gives 
\begin{definition}
    A hypothesis \(\calH\) is \textbf{agnostic PAC learnable} with respect to \(Z\) and a loss function \(l:\calH \times Z \to \Reals_+\), if there exist a function \(m_{\calH} : \opop{0}{1}^2 \to \Naturals\) and a learning alogrithm such that:
    \begin{itemize}
        \item For every \(\epsilon,\delta \in \opop{0}{1}\), distribution \(\calZ\) over \(Z\)
        \item Then, when running the algorithm on \(m \geq \func{m_{\calH}}{\epsilon,\delta}\) of i.i.d. samples generated by \(\calZ\) and labeled by \(f\), the algorithm returns a hypothesis \(h\) such that 
        \begin{equation*}
            \prob{\func{L_{\calZ}}{h} \leq \min_{h'} \func{L_{\calZ}}{h'} + \epsilon} \geq 1- \delta
        \end{equation*}
    \end{itemize}
\end{definition}

\begin{remark}
    In some situations, \(\calH\) is a subset of a set \(\calH'\), and the loss function can be naturally extended to be a function from \(\calH' \times Z\). In this cases, we may allow the algorithm to return a hypothese \(h' \in \calH'\) as long as it satisfies the requirement 
    \begin{equation*}
        \prob{\func{L_{\calZ}}{h'} \leq \min_{h \in \calH} \func{L_{\calZ}}{h} + \epsilon} \geq 1- \delta
    \end{equation*}
    This is called \textit{representation independent} learning, or \textit{improper learning}.
\end{remark}