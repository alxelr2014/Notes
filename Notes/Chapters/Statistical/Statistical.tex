\chapter{Statistical Learning}
A statistical learner needs to know the \textit{domain set}, \(\calX\), \textit{label set}, \(\calY\), and a training data set (more like a sequence) \(S \subset \calX \times \calY\). Given these, learner outputs a \textit{predictor} \(h : \calX \to \calY\) which is also called \textit{hypothesis} or \textit{classifier}. We can assume that \(\calD = \Probability_{\calX}\) is the distribution on \(\calX\) and there exists a correct function \(f\) that for each sampled \(x\) output the corresponding label \(y = \func{f}{x}\). Then the \textit{error} of \(h\) is defined as 
\begin{equation*}
    \func{L_{\calD, f}}{h} = \prob{\func{h}{x} \neq \func{f}{x}}
\end{equation*}

Since we know neither \(f\) nor \(\calD\) we can not find the exact error. To approximate this error, we can use the \textit{empirical error}.
\begin{equation*}
    \func{L_S}{h} = \dfrac{\abs{\set<i>{\func{h}{x_i} \neq y_i }}}{\abs{S}}
\end{equation*}
Since \(S\) is a representation of the real distribution it makes sense to minimize \(\func{L_S}{h}\) and expect that \(\func{L_{\calD,f}}{h}\) is minimized as well. This is called \textbf{empirical risk minimization} or ERM for short. \textit{Overfitting} is one drawback of ERM which arises when \(S\) is not fully representitive of \(\calD\). In that case, predictor though working well on the training data, fails to generalize and mislabels the new data. 

One way to avoid overfitting is to restrict possible hypotheses to a class of hypothese \(\calH\). Then 
\begin{equation*}
    \func{ERM_{\calH}}{S} \in \argmin_{h \in \calH} \func{L_S}{h}
\end{equation*}
This way, we increase the bias toward \(\calH\) and possibly increasing the true error.

\section{Finite hypothesis class}
Suppose \(\calD\) is finite and assume that there exists a \(h^{\ast} \in \calH\) such that 
\begin{equation*}
    \func{L_{\calD,f}}{h^{\ast}} = 0
\end{equation*}
This is called the \textit{realizability assumption}. Furthermore, we can assume that training data are selected independent of each other. 

We often assign a probability\(\delta\) to getting a non-representitive training data. \(1- \delta\) is called the \textit{confidence parameter}. We then assign an \textit{accuracy parameter} \(\epsilon\) where \(\func{L_{\calD, f}}{h_S} > \epsilon\) is a failure. We wish the find an upperbound for the probability of getting a training data that results in a failure. 
\begin{equation*}
    \prob{S \ \suchThat \func{L_{\calD, f}}{h_S} > \epsilon}
\end{equation*}
Let \(\calH_B\) be the set of bad hypotheses
\begin{equation*}
    \calH_B = \set<h>{\func{L_{\calD, f}}{h} > \epsilon}
\end{equation*}
and \(M\) the set of misleading samples 
\begin{equation*}
    M = \set<S>{\exists h \in \calH_B, \; \func{L_{S}}{h} = 0}
\end{equation*}
