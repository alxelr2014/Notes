\chapter{Statistical Learning}
A statistical learner needs to know the \textit{domain set}, \(\calX\), \textit{label set}, \(\calY\), and a training data set (more like a sequence) \(S \subset \calX \times \calY\). Given these, learner outputs a \textit{predictor} \(h : \calX \to \calY\) which is also called \textit{hypothesis} or \textit{classifier}. We can assume that \(\calD = \Probability_{\calX}\) is the distribution on \(\calX\) and there exists a correct function \(f\) that for each sampled \(x\) output the corresponding label \(y = \func{f}{x}\). Then the \textit{error} of \(h\) is defined as 
\begin{equation*}
    \func{L_{\calD, f}}{h} = \prob{\func{h}{x} \neq \func{f}{x}}
\end{equation*}

Since we know neither \(f\) nor \(\calD\) we can not find the exact error. To approximate this error, we can use the \textit{empirical error}.
\begin{equation*}
    \func{L_S}{h} = \dfrac{\abs{\set<i>{\func{h}{x_i} \neq y_i }}}{\abs{S}}
\end{equation*}
Since \(S\) is a representation of the real distribution it makes sense to minimize \(\func{L_S}{h}\) and expect that \(\func{L_{\calD,f}}{h}\) is minimized as well. This is called \textbf{empirical risk minimization} or ERM for short. \textit{Overfitting} is one drawback of ERM which arises when \(S\) is not fully representitive of \(\calD\). In that case, predictor though working well on the training data, fails to generalize and mislabels the new data. 

One way to avoid overfitting is to restrict possible hypotheses to a class of hypothese \(\calH\). Then 
\begin{equation*}
    \func{ERM_{\calH}}{S} \in \argmin_{h \in \calH} \func{L_S}{h}
\end{equation*}
This way, we increase the bias toward \(\calH\) and possibly increasing the true error.

\section{Finite hypothesis class}
Suppose \(\calD\) is finite and assume that there exists a \(h^{\ast} \in \calH\) such that 
\begin{equation*}
    \func{L_{\calD,f}}{h^{\ast}} = 0
\end{equation*}
This is called the \textit{realizability assumption}. Furthermore, we can assume that training data are selected independent of each other. 

We often assign a probability\(\delta\) to getting a non-representitive training data. \(1- \delta\) is called the \textit{confidence parameter}. We then assign an \textit{accuracy parameter} \(\epsilon\) where \(\func{L_{\calD, f}}{h_S} > \epsilon\) is a failure. We wish the find an upperbound for the probability of getting a training data that results in a failure. 
\begin{equation*}
    \prob{S \ \suchThat \func{L_{\calD, f}}{h_S} > \epsilon}
\end{equation*}
Let \(\calH_B\) be the set of bad hypotheses
\begin{equation*}
    \calH_B = \set<h>{\func{L_{\calD, f}}{h} > \epsilon}
\end{equation*}
and \(M\) the set of misleading samples 
\begin{equation*}
    M = \set<S>{\exists h \in \calH_B, \; \func{L_{S}}{h} = 0}
\end{equation*}

\chapter{Probably Approximately Correct Learning}

\begin{definition}
    A hypothesis class \(\calH\) is PAC learnable if there exist a function \(m_{\calH} : \opop{0}{1}^2 \to \Naturals\) and a learning alogrithm such that:
    \begin{itemize}
        \item For every \(\epsilon,\delta \in \opop{0}{1}\), distribution \(\calD\) over \(\calX\), and labeling function \(f: \calX \to \set{0,1}\)
        \item If the realizable assumption hold with respect to \(\calH,\calD,f\)
        \item Then, when running the algorithm on \(m \geq \func{m_{\calH}}{\epsilon,\delta}\) of i.i.d. samples generated by \(\calD\) and labeled by \(f\), the algorithm returns a hypothesis \(h\) such that 
        \begin{equation*}
            \prob{\func{L_{\calD, f}}{h} \leq \epsilon} \geq 1- \delta
        \end{equation*}
    \end{itemize}
\end{definition}
\begin{remark}
    The minimal function \(m_{\calH}\) determines the sample complexity of learning \(\calH\).
\end{remark}

\begin{corollary}
    Every finite hypothesis class is PAC learnable with sample complexity 
    \begin{equation*}
        \func{m_{\calH}}{\epsilon, \delta} \leq \ceil{\frac{\func{\log}{\abs{\calH}/\delta }}{\epsilon}}
    \end{equation*}
\end{corollary}

Let \(\calJ\) be the joint distribution over \(\calX \times \calY\). Note that, \(\calD\) is the marginal distribution of \(\calJ\). Then we can revise the definition for the true error 
\begin{equation*}
    \func{L_{\calJ}}{h} = \func{\Probability_{(x,y) \sim \calJ}}{\func{h}{x} \neq y}
\end{equation*}
Then given \(\calJ\) the best label prediction function is 
\begin{equation*}
    \func{f_{\calJ}}{x} = \begin{cases}
        1 & \text{if} \ \condProb{y = 1}{x} \geq \frac{1}{2}\\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}
that is, there is no other classifier \(g\) with \(\func{L_{\calJ}}{g} < \func{L_{\calJ}}{f_{\calJ}}\)

\begin{definition}
    A hypothesis \(\calH\) is \textbf{agnostic PAC learnable} if there exist a function \(m_{\calH} : \opop{0}{1}^2 \to \Naturals\) and a learning alogrithm such that:
    \begin{itemize}
        \item For every \(\epsilon,\delta \in \opop{0}{1}\), distribution \(\calJ\) over \(\calX \times \calY\)
        \item Then, when running the algorithm on \(m \geq \func{m_{\calH}}{\epsilon,\delta}\) of i.i.d. samples generated by \(\calD\) and labeled by \(f\), the algorithm returns a hypothesis \(h\) such that 
        \begin{equation*}
            \prob{\func{L_{\calJ}}{h} \leq \min_{h'} \func{L_{\calJ}}{h'} + \epsilon} \geq 1- \delta
        \end{equation*}
    \end{itemize}
\end{definition}

\section{Generalized loss functions}
Given any set \(\calH\) and some domain \(Z\), let \(l\) be any function from \(\calH \times Z\) to \(\Reals_+\). We call such functions \textit{loss functions}. We then define the risk function to be
\begin{equation*}
    \func{L_{\calZ}}{h} = \expected[\calZ]{\func{l}{h,z}}
\end{equation*}
where \(h \in \calH\), and \(\calZ\) is the distribution on \(Z\). Similarly, the empirical risk over a given sample \(S \in Z^m\) is 
\begin{equation*}
    \func{L_S}{h} = \dfrac{1}{m} \sum_{i = 1}^m \func{l}{h,z_i}
\end{equation*}
Then revising the agnostic PAC learnability definition for general loss function gives 
\begin{definition}
    A hypothesis \(\calH\) is \textbf{agnostic PAC learnable} with respect to \(Z\) and a loss function \(l:\calH \times Z \to \Reals_+\), if there exist a function \(m_{\calH} : \opop{0}{1}^2 \to \Naturals\) and a learning alogrithm such that:
    \begin{itemize}
        \item For every \(\epsilon,\delta \in \opop{0}{1}\), distribution \(\calZ\) over \(Z\)
        \item Then, when running the algorithm on \(m \geq \func{m_{\calH}}{\epsilon,\delta}\) of i.i.d. samples generated by \(\calZ\) and labeled by \(f\), the algorithm returns a hypothesis \(h\) such that 
        \begin{equation*}
            \prob{\func{L_{\calZ}}{h} \leq \min_{h'} \func{L_{\calZ}}{h'} + \epsilon} \geq 1- \delta
        \end{equation*}
    \end{itemize}
\end{definition}

\begin{remark}
    In some situations, \(\calH\) is a subset of a set \(\calH'\), and the loss function can be naturally extended to be a function from \(\calH' \times Z\). In this cases, we may allow the algorithm to return a hypothese \(h' \in \calH'\) as long as it satisfies the requirement 
    \begin{equation*}
        \prob{\func{L_{\calZ}}{h'} \leq \min_{h \in \calH} \func{L_{\calZ}}{h} + \epsilon} \geq 1- \delta
    \end{equation*}
    This is called \textit{representation independent} learning, or \textit{improper learning}.
\end{remark}