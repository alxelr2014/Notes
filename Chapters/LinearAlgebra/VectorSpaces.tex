\chapter{Vector Spaces}
\thispagestyle{headings}

A \textbf{vector space}, also called \textbf{linear space}, consist of the followings:
\begin{enumerate}
    \item a field \(\Field\) of scalars.
    \item a set \(V\) of vectors.
    \item a vector addition \(+\), with the following properties:
          \begin{enumerate}
              \item \(V\) is closed under addition.
              \item addition is commutative.
              \item addition is associative.
              \item addition has a unique identity element \(0\).
              \item for each vector \(\alpha \in V\), \(\exists \beta \in V \; \suchThat \; a +b = 0\).
          \end{enumerate}
    \item a scalar multiplication with the following properties:
          \begin{enumerate}
              \item \(V\) is closed under scalar multiplication.
              \item \((c_1c_2)\alpha = c_1(c_2 \alpha)\).
              \item \(c(\alpha + \beta) = c \alpha + c \beta\).
              \item \((c_1 + c_2) \alpha = c_1 \alpha + c_2 \alpha\)
              \item scalar multiplication has a unique identity element \(1\).
          \end{enumerate}
\end{enumerate}

\begin{example}
    \(V = \Reals\) and \(\Field = \Reals\) is a vector space. Furthermore, \(V = \Reals^n\) over \(\Field = \Reals\) is a vector space with the scalar multiplication \(c (x_1,x_2,\dots,x_n) = (cx_1,cx_2,\dots ,cx_n)\).
\end{example}

\begin{example}
    \(V = \Reals\) and \(\Field = \Integers\) is not a vector space as \(\Integers\) is not a field.
\end{example}

\begin{definition}
    A vector \(\beta \in V\) is said to be a \textbf{linear combination} of the vectors \(\alpha_1, \alpha_2, \dots, \alpha_n\) if there exists scalars \(c_1, c_2, \dots, c_n \in \Field\) such that:
    \begin{equation*}
        \beta = c_1 \alpha_1 + c_2 \alpha_2 + \dots + c_n \alpha_n = \sum_{i = 1}^{n} c_i \alpha_i
    \end{equation*}
\end{definition}

\section{Subspaces}
Let \(V\) be a vector space over field \(\Field\). A susbspace of \(V\) is a subset \(W\) of \(V\) which is itself a vector space over \(\Field\) with the operations of vector addition and scalar multiplication on \(V\).

\begin{theorem}
    A non-empty subset \(W\) of \(V\) is a subspace of \(V\) if and only if for each pair of vectors \(\alpha, \beta \in W\) and each scalar \(c \in \Field\) the vector \(c\alpha + \beta \in W\).
\end{theorem}

\begin{proof}
    Necessity: Suppose \(W\) is a non-empty subset of \(V\) with the above property. Since \(W\) is not empty then there exists a vector \(\alpha \in W\) and therefore, \((-1) \alpha + \alpha = 0 \in W\). For each \(c \in \Field\), \(c\alpha + 0 = c\alpha \in W\). Finally, if \(\beta \in W\) as well then \(1 \alpha + \beta \in W\). Therefore, \(W\) satisfies all the conditions and is a linear subspace of \(V\).

    Sufficiency: If \(W\) is a subspace of \(V\) and \(\alpha, \beta \in W\) with \(c \in \Field\) then \(c\alpha + \beta \in W\).
\end{proof}

\begin{corollary} \label{th:IntersectionSubspace}
    Let \(V\) be a vector space over \(\Field\). The intersection of any collection subspaces of \(V\) is a subspace of \(V\).
\end{corollary}

\begin{theorem}
    Let \(V\) be a vector space and \(W_1\) and \(W_2\) be two subspaces of \(V\) such that \(W_1 \cap W_2\) is a subspace of \(V\). Then \(W_1 \subset W_2 \) or \(W_2 \subset W_1\).
\end{theorem}

\begin{proof}
    For the sake of contradiction assume neither \(W_1 \subset W_2\) nor \(W_2 \subset W_1\). Then, there are vectors \(\alpha_1\) and \(\alpha_2\) such that \(\alpha_1 \in W_1\) but \(\alpha_2 \notin W_2\) and similarly \(\alpha_2 \in W_2\) but \(\alpha_2 \notin W_1\). Since \(\alpha_1 + \alpha_2 \in W_1 \cup W_2\), then \(\alpha_1 + \alpha_2 \in W_1\) or \(\alpha_1 + \alpha_2 \in W_2\) which is a contradiction.
\end{proof}

\begin{theorem}
    Let \(V\) be a vector space over the inifinite field \(\Field\). If \(W_1,W_2,\dots, W_n\) are subspaces of \(V\) and \(V \subset \cup W_i\), then there exists \(k\) such that \(V = W_k\).
\end{theorem}

\begin{proof}
    too long, do it urself bitch.
\end{proof}

\section{Span}

\begin{definition}
    Let \(S\) be a set of vector in a vector space \(V\). The subspace spanned by \(S\) is defined to be the intersection of all subsapces of \(V\) which contains \(S\) and is denoted by \(\vspan S\). That is, \(\vspan S = \bigcap_{S \subset W} W \). By \Cref{th:IntersectionSubspace}, \(\vspan S\) is a linear subspace. Obviously, \(\vspan S\) is the smallest subspace containing \(S\) because if there were \(S \subset K \subset \vspan S\) since by definition \(\vspan S = \cap W \subset K\), then \(K = \vspan S\).
\end{definition}

\begin{example}
    Let \(S = \{0\}\) then \(\vspan S =  \bigcap_{\{0\} \subset W} W = \{0\}\). Moreover:
    \begin{equation*}
        \vspan \emptyset = \{0\} \qquad \qquad \vspan V = V
    \end{equation*}
\end{example}

\begin{theorem} \label{th:LinearitySpan}
    Let \(V\) is a vector space and \( S \neq \emptyset\)
    \begin{equation*}
        \vspan S = \left \{ c_1 \alpha_1 + \dots + c_n \alpha_n \; \middle| \; \alpha_i \in S, \; c_i \in \Field, \; n \in \natural \right \}
    \end{equation*}
\end{theorem}

\begin{proof}
    Let \(L\) be the set describe above. Clearly, \(S \subset L\) and \(L\) is a subsapce of \(V\) hence \(\vspan S \subset L\). Since \(S \subset \vspan S\) and \(\vspan S\) is closed under addition and scalar multiplication then \( c_1 \alpha_1 + \dots + c_n \alpha_n \in \vspan S\) hence \(L \subset \vspan S\).
\end{proof}

\begin{definition}
    If \(S_1, S_2, \dots , S_k\) are subsets of a vector space \(V\), the set of all sums
    \begin{equation*}
        \alpha_1 + \alpha_2 + \dots + \alpha_k
    \end{equation*}
    where \(\alpha_i \in S_i\) is called the \textbf{sum} of subsets \(S_1, S_2, \dots , S_k\) and is denoted by
    \begin{equation*}
        S_1 + \dots + S_k = \left \{\alpha_1 + \alpha_2 + \dots + \alpha_k \; \middle | \; \alpha_i \in S_i \right \}
    \end{equation*}
\end{definition}

\begin{theorem}
    Let \(W_1, W_2, \dots , W_k\) be subspaces of vector space \(V\). Then
    \begin{equation*}
        W = W_1 + W_2 + \dots + W_k
    \end{equation*}
    is a subspaces of \(V\). Moerover, \( W = \vspan \cup W_i \).
\end{theorem}

\begin{proof}
    Let \(\alpha \in \vspan \cup W_i\) and \(\beta \in W\). By \Cref{th:LinearitySpan}, \(\alpha = \alpha_1 + \dots + \alpha_n\) where \(\alpha_i \in \cup W_i\). Define \(\alpha_1 ' = \sum _{\alpha_i \in W_1} \alpha_i\), \(\alpha_2' = \sum_{\alpha_i \in W_2 \backslash W_1} \alpha_i\) and so on. Clearly, \(\alpha = \alpha_1' + \alpha_2' + \dots + \alpha_k'\) thus \(\alpha \in W\) and \(\vspan \cup W_i \subset W\).
    By definition, \(\beta = \beta_1 + \dots + \beta_k\) where \(\beta_i \in W_i\) and therefore, \(\beta_i \in \cap W_i \subset \vspan \cup W_i\). Since \(\vspan \cup W_i\) is a subspace of \(V\) then \(\beta \in \vspan \cup W_i\) which implies \(W = \vspan \cup W_i\).
\end{proof}

\section{Basis and Dimension}
\begin{definition}[Linearly dependent]
    A set \(S\) of vector space \(V\) is said to be \textbf{linearly dependent}if there are \(c_1, \dots , c_n \in \Field\) where at least one of the \(c_i\) is non-zero and \(\alpha_1, \dots , \alpha_n\) such that
    \begin{equation*}
        c_1 \alpha_1 + \dots + c_n \alpha_n = 0
    \end{equation*}
    Furthermore, \(S\) is \textbf{linearly independent} if it is not linearly dependent. That is, if \(c_1 \alpha_1 + \dots + c_n \alpha_n = 0\) then \(c_i = 0\) for all \(i = 1 , \dots , n\).
\end{definition}

\begin{corollary}
    If \(S\) is linearly dependent and \(S \subset S'\) then \(S'\) is linearly dependent as well. If \(S\) is linearly independent and \(S' \subset S\) then \(S'\) is linearly independent.
\end{corollary}

\begin{definition}[Basis]
    Let \(V\) be a vector space over field \(\Field\). \(S \subset V\) is a \textbf{basis} of \(V\) if it is linearly indepenent and spans \(V\). Additionally, \(V\) is finite dimensional if \(V\) has a finite basis.
\end{definition}

\begin{theorem} \label{th:BasisUpperLimit}
    Let \(V\) be a vector space which is spanned by a \(\beta_1 ,\dots , \beta_m\). Then any independent subset of \(V\) contains at most \(m\) elements.
\end{theorem}

\begin{proof}
    needs matrices maybe.
\end{proof}

\begin{corollary} \label{col:EqualityOfBasis}
    If \(V\) is a finite dimensional vector space then any two bases of \(V\) have the same finite number of elements.
\end{corollary}

\begin{proof}
    Let \(\alpha, \beta \) be two bases of \(V\). By \Cref{th:BasisUpperLimit}, \(\abs{\alpha} \leq \abs{\beta}\) and \(\abs{\beta} \leq \abs{\alpha}\) therefore they must have the same number of elements. Since, there exists a finite basis \(\gamma\) then \(\alpha\) and \(\beta\) must be finite as well.
\end{proof}

\begin{definition}[Dimension]
    \textbf{Dimension} of a finite dimensional vector space \(V\), denoted by \( \dim V\) is the number of elements of one of its basis.
\end{definition}

\begin{example}
    \(\dim \{0\} = 0\) since \(\vspan \emptyset = \{0\}\) and therefore \(\dim \{0\} = \abs[\emptyset ]= 0\).
\end{example}

\begin{corollary}
    Let \(V\) be a finite dimensional vector space and let \( n = \dim V\). Then
    \begin{enumerate}
        \item any subset of \(V\) whichi has more then \(n\) elements is linearly dependent.
        \item no subset of \(V\) which contains fewer than \(n\) vectors can span \(V\).
    \end{enumerate}
\end{corollary}

\begin{proof}
    \begin{enumerate}
        \item It is an immediate consequence of \Cref{th:BasisUpperLimit}.
        \item If there was a set \(\beta = \{\beta_1 , \dots , \beta_m\}\) with \(m < n\) such that \(V = \vspan \beta\) then there must be a basis \(\beta'\) such that \(\abs[\beta'] \leq m\). Which contradicts \Cref{col:EqualityOfBasis}
    \end{enumerate}
\end{proof}

\begin{lemma} \label{lm:MakingBasis}
    Let \(S\) be a linearly independent subset of vector space \(V\). Suppose \(\beta\) is a vector in \(V\) not spanned by \(S\). Then the set \(S \cap \{\beta\}\) is linearly independent.
\end{lemma}

\begin{proof}
    Suppose \(\alpha_1, \dots ,\alpha_m \) are distinct vectors in \(S\) and that
    \begin{equation}
        c_1 \alpha_1 + \dots + c_m \alpha_m + b \beta = 0
    \end{equation}
    then if \(b \neq 0\)
    \begin{equation*}
        \beta =-\dfrac{1}{b} (c_1 \alpha_1 + \dots + c_m \alpha_m )
    \end{equation*}
    and thus \(\beta \in \vspan S\) which is a contradiction.
\end{proof}

\begin{theorem}
    If \(W\) is a subspace of a finite dimensional vector space \(V\) then every linearly independent subset of \(W\) is finite and part of a basis of \(V\).
\end{theorem}

\begin{proof}
    Clearly, since \(W \subset V\) then every linearly independent subset \(S\) of \(W\) must be finite and \(\abs[S] \leq \dim V\). Let \(S = \{\alpha_1, \alpha_m\} \subset W\) be linearly independent. If \(\vspan S = V\) we're done otherwise, there is \(\alpha_{m+1} \in V\) such that \(\alpha_{m+1} \notin \vspan S\). By \Cref{lm:MakingBasis}, \(S_1 = S \cup \{\alpha_{m+1}\}\) is linearly independent. Now if \(\vspan S_1 = V\) we are done. If not we continue in similar fashion. Since \(V\) is finite and any independent set can not have more than \(\dim V\) elements, then in \(\dim V - m\) steps the set \(S \cap \{\alpha_{m+1}, \dots , \alpha_n\}\) is a basis for \(V\).
\end{proof}

\begin{corollary}
    A proper subspace \(W\) of a finite dimensional vector space \(V\) is finite dimensional and \(\dim W < \dim V\).
\end{corollary}

\begin{proof}
    Any basis \(W\) is linearly independent subset of \(V\) and therefore it is finite.Suppose \(W\) has basis \(\{\beta_1, \dots , \beta_m\}\) with \(m \leq \dim V\). Since \(\vspan \beta = W \subsetneq V\) then there is  \(\alpha in V\) that is not in \(W\) therefore, \(\beta \cup \{\alpha\}\) is a linearly independent subset of \(V\) by \Cref{lm:MakingBasis}. Simply
    \begin{equation*}
        \abs[\beta] = \abs[\beta \cup \{\alpha\}] + 1 \leq n
    \end{equation*}
    and thus \(\dim W < \dim V\).
\end{proof}

\begin{theorem}
    If \(W_1\) and \(W_2\) are finite dimensional subspaces of \(V\) (not necessarily finite dimensional), then \(W_1 + W_2\) is finite dimesional and
    \begin{equation*}
        \func{\dim}{W_1 + W_2} = \dim W_1 + \dim W_2 - \func{\dim}{W_1 \cap W_2}
    \end{equation*}
\end{theorem}

\begin{proof}
    \(W_1 \cap W_2\) is a finite subsapce of \(V\) and thus there are \( \alpha = \{\alpha_1, \dots , \alpha_a\}\) which form a basis for \(W_1 \cap W_2\). Furthermore, since \(W_1 \cap W_2 \subset W_1, W_2\) then there are vectors \(\beta = \{\beta_1 , \dots , \beta_b\}\) and \(\gamma = \{\gamma_1 , \dots , \gamma_c\}\) such that \(\alpha \cup \beta\) is a basis for \(W_1\) and \(\alpha \cup \gamma\) is a basis for \(W_2\). We claim that \(\alpha \cup \beta \cup \gamma\) is a basis for \(W_1 + W_2\). For all \(\xi_1 \in W_1\) and \(\xi_2 \in W_2\), \(\xi_1 + \xi_2 \in W_1 + W_2\). Writing \(\xi_1 = x_1 \alpha_1 + \dots + x_a \alpha_a + y_1 \beta_1 + \dots + y_b \beta_b\) and \(\xi_2 = x'_1 \alpha_1 + \dots x'_a \alpha_a + z_1 \gamma_1 + \dots + z_c \gamma_c\) we get
    \begin{equation*}
        \xi_1 + \xi_2 = \sum_{i = 1}^{a} {(x_i + x'_i)\alpha_i} +  \sum_{i = 1}^{b} {y_i \beta_i } +  \sum_{i = 1}^{c} {z_i \gamma_i} \in \func{\vspan}{\alpha \cup \beta \cup \gamma}
    \end{equation*}

    and thus \(W_1 + W_2 \subset \func{\vspan}{\alpha \cup \beta \cup \gamma}\). Cleary \(\func{\vspan}{\alpha \cup \beta \cup \gamma} \subset W_1 + W_2\) and thus \(W_1 + W_2 = \func{\vspan}{\alpha \cup \beta \cup \gamma}\). Now we need to show that \(\alpha \cup \beta \cup \gamma\) is independent. Consider
    \begin{align*}
                 & \sum_{i = 1}^{a} {x_i \alpha_i} +  \sum_{i = 1}^{b} {y_i \beta_i } +  \sum_{i = 1}^{c} {z_i \gamma_i}  = 0 \\
        \implies & \sum_{i = 1}^{a} {x_i \alpha_i} +  \sum_{i = 1}^{b} {y_i \beta_i } = \sum_{i = 1}^{c} {z_i \gamma_i}
    \end{align*}

    and thus \(\sum_{i = 1}^{c} {z_i \gamma_i} \in W_1\) and as a result \(\sum_{i = 1}^{c} {z_i \gamma_i} \in W_1 \cap W_2\) therefore, writing \(\sum_{i = 1}^{c} {z_i \gamma_i} = \sum_{i = 1}^{a} {x'_i \alpha_i}\)
    \begin{align*}
                 & \sum_{i = 1}^{a} {x_i \alpha_i} +  \sum_{i = 1}^{b} {y_i \beta_i } = -\sum_{i = 1}^{a} {x'_i \alpha_i} \\
        \implies & \sum_{i = 1}^{a} {(x_i + x'_i )\alpha_i} +  \sum_{i = 1}^{b} {y_i \beta_i } = 0
    \end{align*}
    and since \(\alpha \cup \beta\) is a basis for \(W_1\) then \(y_i = 0\). hence

    \begin{equation*}
        \sum_{i = 1}^{a} {x_i \alpha_i} + \sum_{i = 1}^{c} {z_i \gamma_i} = 0
    \end{equation*}
    which implies \(\alpha_i = 0, \gamma_i = 0\) as \(\alpha \cup \gamma\) is a basis for \(W_2\).
\end{proof}