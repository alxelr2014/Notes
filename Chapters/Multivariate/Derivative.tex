\chapter{Differentiation}
Let \(V,W\) be finite dimensional vector spaces and \(f: U \subset V \to W\) where \(U\) is open. Then \(f\) is differentiable at \(x_0\) when a linear transformation \(T : V \to W\) such that
\begin{equation*}
    \lim_{\norm{h} \to 0} \dfrac{\norm{\func{f}{x_0 + h} - \func{f}{x_0} - \func{T}{h}}}{\norm{h}}= 0
\end{equation*}
Or equivalently there exists a sublinear function \(\func{R}{h}\) such that
\begin{equation*}
    \func{f}{x_0 + h} - \func{f}{x_0} - Th = \func{R}{h} \qquad \frac{\func{R}{h}}{\norm{h}} \to 0
\end{equation*}
\(T\) if it exists is unique, represented by \(\func{f'}{x_0}\), \(\DiffOperator f\), or \(\func{\diffOperator f}{x}\) and called the \textbf{total derivative} or \textbf{Fr\'{e}chet derivative}.

\begin{example}
    Any linear function \(f : V \to W\) with \(\func{f}{v} = Tv + b\) where \(b \in W\) is differentiable and \(\func{\DiffOperator f}{v} = T\). Since
    \begin{equation*}
        \norm{h}_V < \delta \implies \norm{\func{f}{v + h} - \func{f}{v} - \operatorFunc{\DiffOperator \func{f}{v}}{h}}_W = \norm{T(v+h) - Tv - Th}_W = 0 < \epsilon \norm{h}_V
    \end{equation*}
    Hence, the derivative of any linear function is constant.
    Consider \(S : V \times V \to V\) with \(\func{S}{v,u} = v + u\). \(S\) is differentiable because \(S\) is linear (why?). We claim that \(\DiffOperator S = S\) as
    \begin{equation*}
        \norm{\func{S}{(v + h),(u + k)} - \func{S}{v,u} - \func{S}{h,k}} = 0
    \end{equation*}
\end{example}

\begin{example}
    Let \(\mu : \Reals \times V \to V\) with \(\func{\mu}{r,x} = rx\). Then \(\mu\) is differentiable and \(\operatorFunc{\func{\DiffOperator \mu}{r,x}}{t,h} = rh + tx\) as
    \begin{align*}
        \norm{\func{\mu}{(r + t),(x + h)} - \func{\mu}{r,x} - \operatorFunc{\func{\DiffOperator \mu}{r,x}}{t,h}} & = \norm{rx + rh + tx + th - rx - rh - tx}     \\
                                                                                                                 & = \abs{t} \norm{h} \leq \epsilon \norm{(t,h)}
    \end{align*}
    by letting \(\norm{(t,h)} = \sqrt{t^2 + \norm{h}^2}\) and \(\delta = \epsilon\).
\end{example}

\begin{proposition}
    Differentiability of \(f\) at \(x\) implies continuity at \(x\).
\end{proposition}

\begin{proof}
    \begin{equation*}
        \norm{\func{f}{x + h} - \func{f}{x}} = \norm{\operatorFunc{\DiffOperator \func{f}{x}}{h} + \func{R}{h}} \leq \norm{\DiffOperator \func{f}{x}}\norm{v} + \norm{\func{R}{v}} \to 0
    \end{equation*}
    as \(v \to 0\).
\end{proof}

\begin{proposition} \label{eq:partialDerivative}
    Assume \(f: U \subset V \to W\) is differentiable at \(x_0\) and let \(u \in V\) be a non-zero vector then
    \begin{equation*}
        \func{f'}{x_0} (u) = \lim_{t \to 0} \dfrac{\func{f}{x_0 + tu} - \func{f}{x_0}}{t}
    \end{equation*}
\end{proposition}

\begin{proof}
    Let \(h = tu\) then
    \begin{align*}
        \func{R}{tu}                     & = \func{f}{x_0 + tu} - \func{f}{x_0} - \func{T}{tu}            \\
                                         & = \func{f}{x_0 + tu} - \func{f}{x_0} - t\func{T}{u}            \\
        \implies \dfrac{\func{R}{tu}}{t} & = \dfrac{ \func{f}{x_0 + tu} - \func{f}{x_0}}{t} - \func{T}{u} \\
        \implies \lim_{t \to 0}          & \dfrac{ \func{f}{x_0 + tu} - \func{f}{x_0}}{t} = \func{T}{u}
    \end{align*}
\end{proof}

\begin{definition}[Directional derivative]
    If we let \(\norm{u} = 1\) then the limit in \Cref{eq:partialDerivative} becomes the \textbf{directional derivative} of \(f\) in the direction of \(u\) and is denoted by \(\DiffOperator_u f\).
\end{definition}

\begin{remark}
    The existence of all directional derivatives of \(f\) doesnt imply its differentiability or even its continuity.
\end{remark}

\begin{remark}
    If \(\DiffOperator f: U \to \func{\calL}{V,W}\) is continuous then each \(\PDiff{f_i}{x_j}\) is continuous. Since
    \begin{equation*}
        \DiffOperator \func{f}{x} = \begin{bmatrix}
            \func{\PDiff{f_1}{x_1}}{x} & \dots  & \func{\PDiff{f_1}{x_n}}{x} \\
            \vdots                     & \ddots &                            \\
            \func{\PDiff{f_m}{x_1}}{x} & \dots  & \func{\PDiff{f_m}{x_n}}{x}
        \end{bmatrix}
    \end{equation*}
    and the reverse is true as well.
\end{remark}

\begin{theorem} \label{th:DifferentiabilityCriteria}
    \(f : V \to W\) has all of its partial derivative in a neighbourhood of \(u \in U\) and they're continuous at \(u\) then \(f\) is differentiable at \(u\). Especially, if \(\PDiff{f_i}{x_j}\) exist and are continuous at every point of \(U\) then \(f \in \calC^1\).
\end{theorem}

\begin{proof}
    We prove that each \(f_i\) is differentiable. Let \(\set{e_1, \dots , e_n}\) be a basis for \(V\) and take \(\norm{x} = \sum \abs{\xi_j}\). Consider a convex neighbourhood \(E\) of \(a\). Then, for a given \(\epsilon > 0\) we will show there exists a \(\delta > 0\) such that
    \begin{equation*}
        \norm{h} < \delta \implies \norm{\func{f_i}{a + h} - \func{f_i}{a} - \sum_{j = 1}^n \operatorFunc{\DiffOperator_{e_j} \func{f_i}{a}}{h_j}} \leq \epsilon \norm{h}
    \end{equation*}
    Cosider the point sequence \(a^k =\sum_{j < k} a_j e_j + \sum_{j \geq k} (a_j + h_j)e_j \) where \(a^1 = a + h\) and \(a^{n + 1} = a\) then
    \begin{equation*}
        \norm{\func{f_i}{a + h} - \func{f_i}{a} - \sum_{j = 1}^n \operatorFunc{\DiffOperator_{e_j} \func{f_i}{a}}{h_j}}  \leq \sum_{k = 1}^{n} \norm{\func{f_i}{a^k} - \func{f_i}{a^{k+1}} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a}}{h_k}}
    \end{equation*}
    hence we are done if
    \begin{equation*}
        \norm{\func{f_i}{a^k} - \func{f_i}{a^{k+1}} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a}}{h_k}} \leq \epsilon \abs{h_k}
    \end{equation*}
    for \(k = n\)
    \begin{equation*}
        \norm{\func{f_i}{a^n} - \func{f_i}{a} - \operatorFunc{\DiffOperator_{e_n} \func{f_i}{a}}{h_n}}
    \end{equation*}
    which equivalent to the existence \(n_\cardinalTH\) partial derivative of \(a\). and for \(k < n\)
    \begin{align*}
         & \norm{\func{f_i}{a^k} - \func{f_i}{a^{k+1}} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a}}{h_k}}                                                                                                                                  \\
         & \leq \norm{\func{f_i}{a^k} - \func{f_i}{a^{k+1}} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a^k}}{h_k}}  + \norm{\operatorFunc{\DiffOperator_{e_k} \func{f_i}{a^k}}{h_k} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a}}{h_k}}
    \end{align*}
    which uses the existence of partial derivatives in neighbourhood and its continuity.
\end{proof}

\begin{proposition}
    Let \(f,g : V \to W\) be differentiable at \(x\) and \(h : W \to U\) be differentiable at \(y = \func{f}{x}\). Furthermore, let \(c\) be an scalar then
    \begin{enumerate}
        \item \(\func{\DiffOperator \,}{ f + cg} = \DiffOperator f + c \DiffOperator g\).
        \item  \(h \circ f\) is differentiable at \(x\) and
              \begin{equation*}
                  \func{\DiffOperator \,}{ h \circ f} =  \left( (\DiffOperator h) \circ f \right) \circ \DiffOperator f
              \end{equation*}
    \end{enumerate}
\end{proposition}

\begin{proof} \leavevmode
    \begin{enumerate}
        \item we have
              \begin{align*}
                   & \norm{ \operatorFunc{f + cg}{x + k} - \operatorFunc{f + cg}{x} - \operatorFunc{\DiffOperator \func{f}{x} + c \DiffOperator \func{g}{x}}{k}}                                          \\
                   & \leq \norm{ \func{f}{x + k} - \func{f}{x} - \operatorFunc{\DiffOperator \func{f}{x}}{h}} + \abs{c}\norm{\func{g}{x + k} - \func{g}{x} - \operatorFunc{\DiffOperator \func{g}{x}}{h}}
              \end{align*}
        \item we know that
              \begin{equation*}
                  \begin{cases}
                      \func{f}{x + k} - \func{f}{x} - \operatorFunc{\DiffOperator \func{f}{x}}{k}  = \func{R}{k} \\
                      \func{h}{y + l} - \func{h}{y} - \operatorFunc{\DiffOperator \func{h}{y}}{l}  = \func{S}{l}
                  \end{cases}
              \end{equation*}
              and we wish to prove that
              \begin{equation*}
                  \func{h \circ f}{x + k} - \func{h \circ f}{x} - \operatorFunc{\DiffOperator \func{h}{\func{f}{x}} \circ \DiffOperator \func{f}{x}}{k} = \func{T}{k}
              \end{equation*}
              where \(\norm{\func{T}{k}} \leq \epsilon \norm{k}\) whenever \(\norm{k} < \delta\). Let \(l = \func{f}{x + k} - \func{f}{x}\) and substituting into the second equation
              \begin{align*}
                  \func{h}{\func{f}{x + k}} & - \func{h}{\func{f}{x}} - \operatorFunc{\DiffOperator \func{h}{y}}{\func{f}{x + k} - \func{f}{x}}                                                                                         \\
                                            & =  \func{h}{\func{f}{x + k}} - \func{h}{\func{f}{x}} - \operatorFunc{\DiffOperator \func{h}{y}}{\operatorFunc{\DiffOperator \func{f}{x}}{k}  + \func{R}{k}}                               \\
                                            & = \func{h}{\func{f}{x + k}} - \func{h}{\func{f}{x}} - \operatorFunc{\DiffOperator \func{h}{y} \circ \DiffOperator \func{f}{x}}{k} - \operatorFunc{\DiffOperator \func{h}{y}}{\func{R}{k}} \\
                                            & = \func{T}{k} - \operatorFunc{\DiffOperator \func{h}{y}}{\func{R}{k}} = \func{S}{l}                                                                                                       \\
                  \implies \func{T}{k}      & = \func{S}{l} + \operatorFunc{\DiffOperator \func{h}{y}}{\func{R}{k}}
              \end{align*}
    \end{enumerate}
\end{proof}


\begin{proposition}
    \(f : U \subset V \to W_1 \times \dots \times W_n\) is differentiable at \(x_0\) if and only if all its component is differentiable at \(x_0\). Furthermore, \(\DiffOperator f = (\DiffOperator f_1, \dots , \DiffOperator f_n)\).
\end{proposition}

\begin{proof}
    Define the following norm on \(W_1 \times \dots \times W_n\)
    \begin{equation}
        \norm{(w_1, \dots w_n)} = \sum_{i = 1}^n \norm{w_i}_{W_i}
    \end{equation}
    then
    \begin{equation*}
        \norm{ \func{f}{x_0 + h} - \func{f}{x_0} - \operatorFunc{\DiffOperator \func{f}{a}}{h}} = \sum_{i = 1}^n \norm{ \func{f_i}{x_0 + h} - \func{f_i}{x_0} - \operatorFunc{\DiffOperator \func{f_i}{a}}{h}}
    \end{equation*}
    and since every other norm is equivalent to the norm defined above, we are done.
\end{proof}

\begin{theorem}[Leibnitz rule]
    Let \(V_1, V_2, \dots , V_n\) be finite dimensional vector spaces and \(f: V_1 \times \dots \times V_n \to W\) is a \(n\)-linear function. \(f\) is differentiable at \(a = (a_1, \dots , a_n)\) and
    \begin{equation*}
        \operatorFunc{\func{\DiffOperator f}{a}}{h_1, \dots h_n} = \func{f}{h_1, a_2, \dots, a_n} + \func{f}{a_1, h_2, \dots, a_n} + \dots + \func{f}{a_1, a_2, \dots, h_n}
    \end{equation*}
\end{theorem}

\begin{proof}
    we have that
    \begin{equation*}
        \func{f}{a + h} = \sum_{\xi_i \in \set{a_i,h_i}} \func{f}{\xi_1, \dots , \xi_n}
    \end{equation*}
    therefore
    \begin{equation*}
        \func{f}{a + h} - \func{f}{a} - \sum_{i = 1}^{n} \func{f}{a_1, \dots, a_{i-1}, h_i, a_{i+1}, \dots , a_n} = \sum_{\substack{\xi_i \in \set{a_i,h_i} \\ \text{at least two \(h_i\)}}} \func{f}{\xi_1, \dots , \xi_n}
    \end{equation*}
    Let \(\delta = 1\) then  \(\norm{h} = \sum \norm{h_i} < 1\) also \(i,j, \; \norm{h_i}\norm{h_j} \leq \norm{h}^2\). Hence if we define
    \begin{equation*}
        A = \max \set[\prod_{i \in I} {\norm{a_i}}]{I \subset \Naturals_n}
    \end{equation*}
    then
    \begin{equation*}
        \sum_{\substack{\xi_i \in \set{a_i,h_i} \\ \text{at least two \(h_i\)}}} \func{f}{\xi_1, \dots , \xi_n} \leq (2^n - n - 1)A \norm{h}^2
    \end{equation*}
    and letting \(\delta = \min \set{1 , \dfrac{\epsilon}{(2^n - n - 1)(A + 1)}}\) we arrive at the conclusion.
\end{proof}

\begin{example}
    Let \(Z: \Reals^3 \times \Reals^3 \to \Reals\) with \(\func{Z}{u,v}= u \times v\) be a bilinear function, \(f,g: \Reals \to \Reals^3\) and \(\func{h}{t} = \func{f}{t} \times \func{g}{t}\). \(h = Z \circ \phi\) where \(\func{\phi}{t} = (\func{f}{t},\func{g}{t})\). Then we have:
    \begin{align*}
        \DiffOperator \func{h}{t} & = \operatorFunc{\DiffOperator Z}{\func{\phi}{t}}\circ \DiffOperator \func{\phi}{t}                             \\
                                  & =  \operatorFunc{\DiffOperator Z}{\func{\phi}{t}} \circ (\DiffOperator \func{f}{t}, \DiffOperator \func{g}{t}) \\
                                  & = \func{Z}{\DiffOperator \func{f}{t}, \func{g}{t}} + \func{Z}{ \func{f}{t}, \DiffOperator \func{g}{t}}         \\
                                  & = \DiffOperator \func{f}{t} \times \func{g}{t} + \func{f}{t} \times \DiffOperator \func{g}{t}
    \end{align*}
\end{example}

\begin{example}
    Consider \(A = [\func{f_{ij}}{x_1, \dots , x_n}]\) where each \(f_{ij}\) is differentiable. Then
    \begin{equation*}
        \DiffOperator \func{\det}{A}
    \end{equation*}
    can be calculated using the Leibnitz rule, since determinant is \(n\)-linear function.
\end{example}

\section{Mean value theorem}
Mean value theroem of 1-dimensional does not generalize very well. For example, the continuous function \(\func{f}{t} : \clcl{0}{1} \to \Reals^2\) with
\begin{equation*}
    t \mapsto (t^2, t^3)
\end{equation*}
is differentiable on \(\opop{0}{1}\), however
\begin{align*}
    \func{f}{1} - \func{f}{0} = (1,1) & = \DiffOperator \func{f}{c} (1 - 0) \\
                                      & = (2c,3c^2)
\end{align*}
which has no solution for \(c \in \opop{0}{1}\).
Although it must be said that for \(f: U \to \Reals\) where \(U \subset V\) is convex, the mean value theorem holds.

\begin{theorem} \label{th:MultivariableMVT}
    Let \(V,W\) be normed finite dimensional vector spaces and \(f: U \to W\) is differentiable and \(A,B \in U\) are such that the line connecting in completely contained in \(U\) and for each \(p\) on that line
    \begin{equation*}
        \norm{\DiffOperator \func{f}{p}} \leq M
    \end{equation*}
    then
    \begin{equation*}
        \norm{\func{f}{B} - \func{f}{A}}_W \leq M \norm{B - A}_V
    \end{equation*}
\end{theorem}
First consider the following lemma:
\begin{lemma} \label{lm:MeanValueTheoremLemma}
    If \(\phi: \clcl{0}{1} \to W\) is continuous, differentiable on \(\opop{0}{1}\) and \(\norm{\func{\phi'}{t}} \leq M\) for all \(t \in \opop{0}{1}\) then
    \begin{equation*}
        \norm{\func{\phi}{1} - \func{\phi}{0}}_W \leq M
    \end{equation*}
\end{lemma}

\begin{prooflemma}
    We provide three proofs for the lemma
    \begin{enumerate}
        \item Assuming the norm on \(W\) is induced by an inner product. Then, let \( e = \frac{\func{\phi}{1} - \func{\phi}{0}}{\norm{\func{\phi}{1} - \func{\phi}{0}}}\) be a unit vector in \(W\) then \(\psi : \clcl{0}{1} \to \Reals\), \(\func{\psi}{t} = e \cdot \func{\phi}{t}\) is continuous and differentiable on \(\opop{0}{1}\). By the mean the value theorem
              \begin{align*}
                  \abs{\func{\psi}{1} - \func{\psi}{0}}                        & = \abs{\func{\psi'}{t_0}}       \\
                  \abs{e \cdot \left( \func{\phi}{1} - \func{\phi}{0} \right)} & = \abs{e \cdot \func{\phi'}{t}} \\
                  \norm{\func{\phi}{1} - \func{\phi}{0}}                       & \leq M
              \end{align*}
        \item Using the Hahn-Banach theorem, that is for a finite dimensional vector space \(V\) and \(e \in V\) with \(\norm{e} = 1\) there exists a linear function \(\theta : V \to \Reals\) such that \(\norm{\theta} = 1\) and \(\func{\theta}{e} = 1\). Now let \(\func{\psi}{t} = \func{\theta}{\func{\phi}{t}}\) and take \(e\) as defined above then
              \begin{align*}
                  \abs{\func{\psi}{1} - \func{\psi}{0}}                & = \abs{\func{\psi'}{t_0}}                                                             \\
                  \abs{\func{\theta}{\func{\phi}{1} - \func{\phi}{0}}} & = \operatorFunc{\DiffOperator \func{\theta}{\func{\phi}{t_0}}}{\func{\phi'}{t_0}}     \\
                  \norm{\func{\phi}{1} - \func{\phi}{0}}               & = \func{\theta}{\func{\phi'}{t_0}} \leq \norm{\theta} \norm{\func{\phi'}{t_0}} \leq M
              \end{align*}
        \item From Hoimander. For any \(\epsilon\) consider the set \(T_\epsilon\).
              \begin{equation*}
                  T_\epsilon = \set[t \in \clcl{0}{1}]{\forall s, \; 0 \leq s \leq t, \; \norm{\func{\phi}{s} - \func{\phi}{0}} \leq(M + \epsilon)s + \epsilon}
              \end{equation*}
              first note that \(T_\epsilon = \clcl{0}{c}\) for some \(c > 0\) because for \(s = 0\) the inequality is strict and both sides are continuous with respect to \(s\). We claim that \(c = 1\) because otherwise \(c < 1\) and by differentiability of \(\phi\), there exists a \(\delta < 1 - c\) such that if
              \begin{align*}
                  \norm{h} < \delta \implies \norm{\func{\phi}{c + h} - \func{\phi}{c} - \operatorFunc{\DiffOperator \func{\phi}{c}}{h}} & \leq \epsilon \norm{h}                                        \\
                  \implies \norm{\func{\phi}{c + h} - \func{\phi}{c}}                                                                    & \leq\norm{h} \left( \epsilon + \norm{\func{\phi'}{c}} \right) \\
                                                                                                                                         & \leq \norm{h} (\epsilon + M)
                  \intertext{also since \(c \in T_\epsilon\)}
                  \norm{\func{\phi}{c} - \func{\phi}{0}}                                                                                 & < (M + \epsilon)c + \epsilon                                  \\
                  \implies \norm{\func{\phi}{c + h} - \func{\phi}{0}}                                                                    & < (M + \epsilon)(c + h) + \epsilon \qquad 0 < h < \delta
              \end{align*}
              hence \(c + h \in T_\epsilon\) which is a contradiction and thus \(c = 1\).
    \end{enumerate}
\end{prooflemma}

\begin{proof}
    Let \(\sigma : \clcl{0}{1} \to U\) be a parameterization of the line connecting the point \(A\) to point \(B\), \linebreak \(\func{\sigma}{t} = (1 - t) A + tB\). Let \(\phi = f \circ \sigma\), then clearly \(\phi\) is continuous on \(\clcl{0}{1}\) and differentiable on \(\opop{0}{1}\) and we have
    \begin{align*}
        \func{\phi'}{t}                 & = \operatorFunc{\DiffOperator \func{f}{\func{\sigma}{t}}}{\func{\sigma'}{t} }         \\
                                        & = \operatorFunc{\DiffOperator \func{f}{\func{\sigma}{t}}}{B - A}                      \\
        \implies \norm{\func{\phi'}{t}} & \leq \norm{\DiffOperator \func{f}{\func{\sigma}{t}}} \norm{B-A}_V \leq M \norm{B-A}_V
    \end{align*}
    therefore by the \Cref{lm:MeanValueTheoremLemma}
    \begin{equation*}
        \norm{\func{f}{B} - \func{f}{A}}_W = \norm{\func{\phi}{1} - \func{\phi}{0}}_W \leq M \norm{B-A}_V
    \end{equation*}
    which concludes the proof.
\end{proof}

\begin{corollary} \label{cr:derivativeZeroConstant}
    Let \(U \subset V\) is connected and open and \(f: U \to W\) is differentiable and \(\DiffOperator \func{f}{u} = 0\) for all \(u \in U\) then \(f\) is constant.
\end{corollary}

\begin{proof}
    Let \(p \in U\) and \( S = \set[q \in U]{\func{f}{q} = \func{f}{p}} \). \(S\) is closed because \(f\) is continuous and hence the pre-image closed set \(\set{\func{f}{p}}\) is closed. For each \(q \in S\) there exists \(r > 0\) such that \(\func{B_r}{q} \subset U\) and since \(\func{B_r}{q}\) is convex then for each \( l \in \func{B_r}{q}\) we apply the \Cref{th:MultivariableMVT}
    \begin{equation*}
        \norm{\func{f}{l} - \func{f}{q}} \leq \sup \norm{\DiffOperator \func{f}{t}} \norm{l - q} = 0
    \end{equation*}
    which implies that \(\func{f}{l} = \func{f}{q} = \func{f}{p}\) hence \(S\) is open in \(U\) which by the connectedness of \(U\) means \(S = U\). Therefore, \(f\) is constant on \(U\).
\end{proof}

\begin{corollary}
    Let \(V_1, V_2, W\) be finite dimensional normed vector space and \(U \subset V_1 \times V_2\) is open such that for every \(y \in V_2\) the intersection \((V_1 \times \set{y}) \cap U\) is connected. Assumne \(f : U \to W\) is differentiable and \(\DiffOperator_{V_1} \func{f}{x,y} = 0\) for all \((x,y) \in U\) then for any two point \((x_1,y), (x_2,y) \in U\),\(\func{f}{x_1,y} = \func{f}{x_2,y}\).
\end{corollary}

\begin{proof}
    Fix \(y \in V_2\) and define the function \(g : V_1 \to W\)
    \begin{equation*}
        \func{g}{x} = \func{f}{x,y}
    \end{equation*}
    therefore
    \begin{equation*}
        \DiffOperator \func{g}{x} = \DiffOperator_{V_1} \func{f}{x,y} = 0
    \end{equation*}
    and since \((V_1 \times \set{y}) \cap U\), the domain of \(g\) is connected. Hence by applying the \Cref{cr:derivativeZeroConstant} we get that
    \begin{equation*}
        \func{g}{x} = c \implies \func{f}{x_1,y} = \func{f}{x_2,y}
    \end{equation*}
    for all \(y \in V_2\).
\end{proof}

\section{Fundamental theorem of calculus}
\begin{theorem}
    Let \(U\) be an open set of \(V\) such that for every \(A,B \in U\) the line segment connecting \(A\) and \(B\) remains in \(U\) and let \(\sigma : \clcl{0}{1}\to U\) be that line, \(\func{\sigma}{t} = (1-t)A + tB\), and lastly let \(f: U \to W\) is continuously differentiable. Then
    \begin{equation*}
        \func{f}{B} - \func{f}{A} = \func{T}{B - A}
    \end{equation*}
    where \(T\) is
    \begin{equation*}
        T = \int_{0}^{1} \DiffOperator \func{f}{ \func{\sigma}{t}} \diffOperator t
    \end{equation*}
\end{theorem}

\begin{proof}
    Let \(g_i : \clcl{0}{1} \to \Reals\) be
    \begin{equation*}
        \func{g_i}{t} = \pi_i \circ \func{f}{\func{\sigma}{t}}
    \end{equation*}
    is continuously differentiable then by the fundamental theorem of calculus for the real-valued functions we have
    \begin{align*}
        \func{g}{1} - \func{g}{0}                                   & = \int_{0}^{1} \func{g'}{t} \diffOperator t                                                                        \\
                                                                    & = \int_{0}^{1} \pi_i \circ \DiffOperator \func{f}{\func{\sigma}{t}} \diffOperator t                                \\
                                                                    & = \pi_i \circ \int_{0}^{1} \DiffOperator \func{f}{\func{\sigma}{t}} \DiffOperator \func{\sigma}{t} \diffOperator t \\
                                                                    & = \pi_i \circ \int_{0}^{1} \DiffOperator \func{f}{\func{\sigma}{t}} (B - A) \diffOperator t                        \\
        \implies \pi_i \circ \left(\func{f}{B} - \func{f}{A}\right) & = \pi_i \circ \func{T}{B- A}                                                                                       \\
        \implies \func{f}{B} - \func{f}{A}                          & =\func{T}{B- A}
    \end{align*}
    which was what was wanted.
\end{proof}

\begin{theorem}
    Consider the continuous function \(T: U \times U \to \func{\calL}{V,W}\) which is such that
    \begin{equation*}
        \func{f}{B} - \func{f}{A} = \operatorFunc{\func{T}{A,B}}{B-A}
    \end{equation*}
    then \(f \in \calC^1\) and \(\DiffOperator \func{f}{A} = \func{T}{A,A}\)
\end{theorem}

\begin{proof}
    We have
    \begin{equation*}
        \func{f}{A + h} - \func{f}{A} = \operatorFunc{\func{T}{A+h,A}}{h}
    \end{equation*}
    hence
    \begin{align*}
        \norm{\func{f}{A + h} - \func{f}{A} - \operatorFunc{\func{T}{A,A}}{h}} & = \norm{ \operatorFunc{\func{T}{A+h,A}}{h} - \operatorFunc{\func{T}{A,A}}{h}} \\
                                                                               & \leq \norm{\func{T}{A+h,A} - \func{T}{A,A}}\norm{h}
    \end{align*}
    now by continuity of \(T\), there exists a \(\delta > 0\) such that
    \begin{equation*}
        \norm{(h,k)} < \delta \implies \norm{\func{T}{A+h,A+k} - \func{T}{A,A}} < \epsilon
    \end{equation*}
    By letting \(k = 0\) we get \(\DiffOperator \func{f}{A} = \func{T}{A,A}\). Since \(T\) is continuous then \(f \in \calC^1\) as well.
\end{proof}

\begin{corollary}
    Let \(V\) be a normed finite dimensional vector space and \(U\) is open subset of \(V\). If
    \begin{equation*}
        f :\clcl{a}{b} \times U \to \Reals
    \end{equation*}
    is continuous then
    \begin{equation*}
        \func{F}{y} = \int_{a}^{b} \func{f}{x,y} \diffOperator x
    \end{equation*}
    is continuous. Furthermore, if \(\PDiff{f}{y_i}\) exists and is continuous then \(\PDiff{F}{y_i}\) exists and is continuous as well.
    \begin{equation*}
        \PDiff{F}{y_i} =  \int_{a}^{b} \func{\PDiff{f}{y_i}}{x,y} \diffOperator x
    \end{equation*}
\end{corollary}

\begin{proof}
    Firstly, we want to show that there exists a \(\delta > 0\) such that for each \(y \in U\)
    \begin{equation*}
        \norm{h} < \delta \implies  \norm{\func{F}{y + h} - \func{F}{y}} < \epsilon
    \end{equation*}
    we have that
    \begin{align*}
        \norm{\func{F}{y + h} - \func{F}{y}} & =  \norm{\int_{a}^{b} \func{f}{x,y+ h} \func{f}{x,y} \diffOperator x}    \\
                                             & \leq (b-a) \sup_{x \in \clcl{a}{b}} \set{\func{f}{x,y+ h} \func{f}{x,y}}
    \end{align*}
    note that from the continuity of \(f\) for each \(x \in \clcl{a}{b}\) and \(y \in U\) there are open balls \(I_{x,y}\) around \(x\) and \(J_{x,y}\) around \(y\) such that
    \begin{equation*}
        x' \in I_{x,y}, \; y' \in J_{x,y} \implies \norm{\func{f}{x',y'} - \func{f}{x,y}} < \dfrac{\epsilon}{b - a}
    \end{equation*}
    Fix \(y_0\), then \(\cup I_{x,y_0} \supset \clcl{a}{b}\) which by the compactness of the interval implies that there is a finite family of there open set the covers \(\clcl{a}{b}\). Setting \(\delta\) to the minimum radius of \(J_{x,y_0}\) yields the result.
    Secondly, we show that there exists a \(\delta > 0 \) such that
    \begin{equation*}
        \abs{h} < \delta \implies \norm{\dfrac{\func{F}{y + he_i} - \func{F}{y}}{h}} < \epsilon
    \end{equation*}
    and we have that
    \begin{align*}
        \dfrac{\func{F}{y + he_i} - \func{F}{y}}{h} & = \dfrac{1}{h}  \int_{a}^{b} \func{f}{x,y+ he_i} - \func{f}{x,y} \diffOperator x   \\
                                                    & =  \dfrac{1}{h}  \int_{a}^{b} \func{\PDiff{f}{y_i}}{x,y + the_i}h  \diffOperator x \\
                                                    & = \int_{a}^{b} \func{\PDiff{f}{y_i}}{x,y + the_i}  \diffOperator x
    \end{align*}
    from the previous part we know that we can make
    \begin{equation*}
        \norm{\func{\PDiff{f}{y_i}}{x,y'} - \func{\PDiff{f}{y_i}}{x,y}}
    \end{equation*}
    as small as we want by making \(\norm{y - y'} < \delta\) small independently of \(x\). Therefore, there exist a \(\delta > 0\) such that if \(\abs{th} < \abs{h} < \delta\) then
    \begin{equation*}
        \norm{\func{\PDiff{f}{y_i}}{x,y'} - \func{\PDiff{f}{y_i}}{x,y}} < \frac{\epsilon}{b-a}
    \end{equation*}
    hence
    \begin{equation*}
        \norm{\dfrac{\func{F}{x,y + he_i} - \func{F}{x,y}}{h} - \int_{a}^{b} \func{\PDiff{f}{y_i}}{x,y} \diffOperator x} = \norm{\int_{a}^{b} \func{\PDiff{f}{y_i}}{x,y + the_i} - \func{\PDiff{f}{y_i}}{x,y}  \diffOperator x}
        < \frac{\epsilon}{b - a}
    \end{equation*}
    and the continuity of \(\PDiff{F}{y_i}\) comes as a result of applying the first part to \(\PDiff{f}{y_i}\).
\end{proof}
\section{Higher derivative}
Let \(V,W\) be finite dimensional normed vector spaces with \((e_1, \dots , e_n)\) is an ordered basis for \(V\). Consider \(U \subset V\) is an open set and \(f: U \to W\). If \(f\) is differentiable then its partial derivatives
\begin{equation*}
    \DiffOperator_i f: U \to E \quad \text{with} \quad \operatorFunc{\DiffOperator_i f}{x} = \operatorFunc{\DiffOperator \func{f}{x}}{e_i}
\end{equation*}
Then, clearly if \(\DiffOperator_i f\) is differentiable one can define its partial derivatives \(\operatorFunc{\DiffOperator_j}{\DiffOperator_i f}\) also denoted by
\begin{equation*}
    \operatorFunc{\DiffOperator_j}{\DiffOperator_i f}=  \dfrac{\PDiffOperator^2 f}{\PDiffOperator x_j \PDiffOperator x_i} =\DiffOperator_{ji} f
\end{equation*}

For Fr\'{e}chet derivative, if \(\DiffOperator f: U \to \func{\calL}{V,W}\) is differentiable at \(x\), then \(f\) is twice differentiable and
\begin{equation*}
    \DiffOperator^2 \func{f}{x} = \operatorFunc{\func{\DiffOperator \,}{\DiffOperator f}}{x} : U \xrightarrow{\text{linear map}} \func{\calL}{V,W}
\end{equation*}
is a linear map. Therefore,
\begin{equation*}
    \DiffOperator^2 f :  U \to \func{\calL}{V,\func{\calL}{V,W}}
\end{equation*}
which by the \Cref{pr:nLinearIsmorphicLinear} is equivalent to \(\func{\calL^2}{V \times V, W}\) and one can define
\begin{equation*}
    \diffOperator^2 f : U  \to \func{\calL^2}{V \times V, W}
\end{equation*}
where \(\diffOperator^2 = \func{T}{\DiffOperator^2}\) as defined in \Cref{pr:nLinearIsmorphicLinear}. With this definition, for the higher order derivatives \(n \geq 2\)
\begin{equation*}
    \diffOperator^n : U \to \func{\calL^n}{V^n, W}
\end{equation*}

\begin{example}
    Let \(A : V \to W\) be a affine function \(\func{A}{x} = Lx + b\) where \(L\) is linear. Then, \(\DiffOperator \func{A}{x} = L\) and hence \(\DiffOperator^2 A = 0\).
\end{example}

\begin{example}
    Let \(\beta : V \times V \to W\) be a bilinear function. By the Leibnitz rule
    \begin{equation*}
        \operatorFunc{\DiffOperator \func{\beta}{x_1,x_2}}{h_1,h_2} = \func{\beta}{x_1,h_2} + \func{\beta}{h_1,x_2}
    \end{equation*}
    therefore \(\DiffOperator \beta: V \times V \to \func{\calL}{V \times V, W}\) is a linear a function itself, since
    \begin{equation*}
        \operatorFunc{\DiffOperator \func{\beta}{x_1 + x'_1,x_2 + x'_2}}{h_1,h_2} = \func{\beta}{x_1,h_2} + \func{\beta}{x'_1,h_2} + \func{\beta}{x_1,h_2} + \func{\beta}{b_1,x'_2}
    \end{equation*}
    which means \(\operatorFunc{\func{\DiffOperator \,}{\DiffOperator \beta}}{x} = \DiffOperator \beta\) independent of \(x\).
\end{example}

\begin{theorem}
    If \(f\) is twice differentiable at \(p\) then its second partial derivatives exist at \(p\). Conversely, if its second partial derivatives exist at a neighbourhood of \(p\) and they are continuous, then \(f\) is differentiable.
\end{theorem}

\begin{proof}
    Assume that \(\DiffOperator^2 \func{f}{p}\) exists. Then
    \begin{align*}
        \func{\DiffOperator_j \,}{\DiffOperator_i \func{f}{p}} & = \lim_{h \to 0} \dfrac{\operatorFunc{\DiffOperator \func{f}{p + he_j}}{e_i} - \operatorFunc{\DiffOperator \func{f}{p}}{e_i} }{h} \\
                                                               & = \operatorFunc{ \lim_{h \to 0} \dfrac{\DiffOperator \func{f}{p + he_j} - \DiffOperator \func{f}{p}}{h} }{e_i}
    \end{align*}
    which exists sincce \(\DiffOperator f\) is differentiable at \(p\). Conversely, assume that the second partials exist and are continuous at \(p\). Then
    \begin{equation*}
        \operatorFunc{\func{\DiffOperator \,}{\DiffOperator f}}{p} = \begin{bmatrix}
            \func{\PDiff{\DiffOperator f}{x_1}}{p} & \dots & \func{\PDiff{\DiffOperator f}{x_n}}{p}
        \end{bmatrix}
    \end{equation*}
    note that each \(\func{\PDiff{\DiffOperator f}{x_i}}{p}\) is in \(\func{\calL}{V,W}\). In fact, since
    \begin{equation*}
        \DiffOperator f = \begin{bmatrix}
            \PDiff{f}{x_1} & \dots & \PDiff{f}{x_n}
        \end{bmatrix}
    \end{equation*}
    then
    \begin{equation*}
        \PDiff{\DiffOperator f}{x_i} =  \begin{bmatrix}
            \dfrac{\PDiffOperator^2 f}{\PDiffOperator x_i \PDiffOperator x_1} & \dots & \dfrac{\PDiffOperator^2 f}{\PDiffOperator x_i \PDiffOperator x_n}
        \end{bmatrix}
    \end{equation*}
    which is continuous at \(p\) and hence \(\func{\PDiff{\DiffOperator f}{x_i}}{p}\) is continuous and by \Cref{th:DifferentiabilityCriteria}, \(\DiffOperator f\) is differentiable at \(p\).
\end{proof}

\begin{remark}
    In general, one can show that \(f \in \calC^r\) is equivalent to its partial being in \(\calC^r\).
\end{remark}

Let \(f: U \to \Field\) then \(\DiffOperator f: U \to \func{\calL}{V,\Field}\) which is the topological dual space \(V^\ast\) therefore
\begin{equation*}
    \DiffOperator f = \sum_{i = 1}^n \PDiff{f}{x_i} e^\ast_i
\end{equation*}
then for the second derivative of \(f\), \(\DiffOperator^2 \func{f}{x} : U \to V^\ast\)
\begin{align*}
    \DiffOperator^2 \func{f}{x}                                                  & = \sum_{i = 1}^n \func{\PDiff{\DiffOperator f}{x_i}}{x} e^\ast_i                                                     \\
                                                                                 & = \sum_{i = 1}^n \func{\PDiff{\sum_{j = 1}^n \PDiff{f}{x_j}}{x_i}}{x}  e^\ast_j e^\ast_i                             \\
                                                                                 & =  \sum_{i = 1}^n \sum_{j = 1}^n \dfrac{\PDiffOperator^2 f}{\PDiffOperator x_i \PDiffOperator x_j} e^\ast_j e^\ast_i \\
    \implies \operatorFunc{\operatorFunc{\DiffOperator^2 \func{f}{x}}{e_i}}{e_j} & = \func{\dfrac{\PDiffOperator^2 f}{\PDiffOperator x_i \PDiffOperator x_j}}{x}                                        \\
    \implies \func{\diffOperator^2 \func{f}{x}}{e_i,e_j}                         & = \func{\dfrac{\PDiffOperator^2 f}{\PDiffOperator x_i \PDiffOperator x_j}}{x}
\end{align*}

\begin{definition}[Hessian matrix]
    If for a function \(f : U \to \Field\) all of its second partial derivatives exist then \textbf{hessian matrix} is
    \begin{equation*}
        \begin{bmatrix}
            \func{\dfrac{\PDiffOperator^2 f}{\PDiffOperator x_1^2}}{x}                  & \dots  & \func{\dfrac{\PDiffOperator^2 f}{\PDiffOperator x_1 \PDiffOperator x_n}}{x} \\
            \vdots                                                                      & \ddots & \vdots                                                                      \\
            \func{\dfrac{\PDiffOperator^2 f}{\PDiffOperator x_n \PDiffOperator x_1}}{x} & \dots  & \func{\dfrac{\PDiffOperator^2 f}{\PDiffOperator x_n^2}}{x}
        \end{bmatrix}
    \end{equation*}
\end{definition}

%TODO: needs better proof
\begin{theorem}
    If \(f\) is twice differentiable at \(x\), \(\diffOperator^2 \func{f}{x}\) is symmetric. That is,
    \begin{equation*}
        \func{\diffOperator^2 \func{f}{x}}{h,k} = \func{\diffOperator^2 \func{f}{x}}{k,h}
    \end{equation*}
\end{theorem}

\begin{proof}
    Let \(\norm{h}\) and \(\norm{k}\) be sufficiently small such that \(a + th, a+  tk , a+th + tk\) and the lines connecting them stays in \(U\) for some \(t \in \Reals\). Consider
    \begin{equation*}
        \func{\Delta}{t,h,k} = \func{f}{a + th + tk} - \func{f}{a + th} - \func{f}{a  + tk} + \func{f}{a}
    \end{equation*}
    Assuming \(f\) is a real-valued twice differentiable function then if we prove
    \begin{equation*}
        \operatorFunc{\diffOperator^2 \func{f}{x}}{h,k} = \lim_{t \to 0} \dfrac{\func{\Delta}{t,h,k}}{t^2}
    \end{equation*}
    we are done since, \(\Delta\) is symmetric with respect to \(h\) and \(k\). Now consider
    \begin{equation*}
        \func{g}{s} = \func{f}{a + th + tsk} - \func{f}{a  + tsk}
    \end{equation*}
    then by the the Mean value theorem
    \begin{align*}
        \func{\Delta}{t,h,k} = \func{g}{1} - \func{g}{0} & = \func{g'}{\xi}                                                                                                       \\
                                                         & =\operatorFunc{ \DiffOperator \func{f}{a + th + t\xi k}}{tk} - \operatorFunc{ \DiffOperator \func{f}{a  + t\xi k}}{tk}
    \end{align*}
    and since \(\DiffOperator f\) is differentiable then by definition
    \begin{align*}
        \implies \DiffOperator \func{f}{a+x} & = \DiffOperator \func{f}{a} + \operatorFunc{\DiffOperator^2 \func{f}{a}}{x} - \func{R}{x}
    \end{align*}
    therefore
    \begin{multline*}
        \func{\Delta}{t,h,k} = t\operatorFunc{\DiffOperator \func{f}{a } + \operatorFunc{\DiffOperator^2 \func{f}{a}}{th + t\xi k} - \func{R}{th + t\xi k}}{k} \\- t\operatorFunc{\DiffOperator \func{f}{a} + \operatorFunc{\DiffOperator^2 \func{f}{a}}{t\xi k} - \func{R}{t\xi k}}{k}
    \end{multline*}
    then
    \begin{align*}
        \func{\Delta}{t,h,k}                       & = t \operatorFunc{\operatorFunc{\DiffOperator^2 \func{f}{a}}{th + t\xi k}-\operatorFunc{\DiffOperator^2 \func{f}{a}}{t\xi k} }{k} - t \operatorFunc{ \func{R}{t\xi k} -\func{R}{th + t\xi k}}{k}               \\
                                                   & = t^2 \operatorFunc{\operatorFunc{\DiffOperator^2 \func{f}{a}}{h}}{k} - t \operatorFunc{ \func{R}{t\xi k} -\func{R}{th + t\xi k}}{k}                                                                           \\
        \implies \dfrac{\func{\Delta}{t,h,k}}{t^2} & = \operatorFunc{\operatorFunc{\DiffOperator^2 \func{f}{a}}{h}}{k} - \dfrac{ \operatorFunc{ \func{R}{t\xi k} -\func{R}{th + t\xi k}}{k}}{t} \to \operatorFunc{\operatorFunc{\DiffOperator^2 \func{f}{a}}{h}}{k}
    \end{align*}
    which is what we wanted.
\end{proof}

%TODO: needs proof
\begin{theorem}
    The \(k_\cardinalTH\) derivative of a \(k\)-times differentiable function is a symmetric \(k\)-linear function.
\end{theorem}

\begin{proof}
    it's generalization of above.
\end{proof}

\begin{proposition}
    If \(f,g \in \calC^r\) are two functions then \(f \circ g \in \calC^r\).
\end{proposition}

\begin{proof}
    Let \(f: V' \to V''\) and \(g: V \to V'\) be two \(\calC^r\) functions and  \(\beta : \func{\calL}{V',V''} \times \func{\calL}{V,V'} \to \func{\calL}{V,V''}\) is a bilinear function such that
    \begin{equation*}
        \func{\beta}{\phi,\psi} = \phi \circ \psi
    \end{equation*}
    Now note that
    \begin{align*}
        \operatorFunc{\func{\DiffOperator \,}{ f \circ g}}{a} & = \operatorFunc{\DiffOperator f \circ g}{a}\circ \DiffOperator \func{g}{a}            \\
                                                              & = \func{\beta }{\operatorFunc{\DiffOperator f \circ g}{a}, \DiffOperator \func{g}{a}}
    \end{align*}
    Consider the following functions
    \begin{equation*}
        a \xmapsto[\calC^\infty]{\Delta} (a,a) \xmapsto[\calC^{r-1}]{(\DiffOperator f \circ g, \DiffOperator g)} (\operatorFunc{\DiffOperator f \circ g}{a}, \DiffOperator \func{g}{a}) \xmapsto[\calC^\infty]{\beta}  \operatorFunc{\func{\DiffOperator \,}{ f \circ g}}{a}
    \end{equation*}
    therefore \(\func{\DiffOperator \,}{ f \circ g} \in \calC^{r-1}\) and hence \(f \circ g \in \calC^{r}\).
\end{proof}

\begin{example}
    The inverse operator \(i : \func{\GL}{V} \to \func{\calL}{V,V}\) is in \(C^\infty\). Remember that
    \begin{equation*}
        \operatorFunc{\operatorFunc{\DiffOperator i}{A}}{M} = - A^{-1}M A^{-1}
    \end{equation*}
    Let \(\gamma : \func{\calL}{V,V} \times \func{\calL}{V,V} \to \func{\calL}{\func{\calL}{V,V} , \func{\calL}{V,V}}\) with
    \begin{equation*}
        \operatorFunc{\func{\gamma}{A,B}}{M} = - AMB
    \end{equation*}
    is a bilinear function. Therefore
    \begin{equation*}
        \operatorFunc{\operatorFunc{\DiffOperator i}{A}}{M} = \operatorFunc{\func{\gamma}{A^{-1},A^{-1}}}{M}
    \end{equation*}
    now
    \begin{equation*}
        A \xmapsto{i} A^{-1} \xmapsto[\calC^\infty]{\Delta} (A^{-1}, A^{-1}) \xmapsto[\calC^\infty]{\gamma} \operatorFunc{\DiffOperator i}{A}
    \end{equation*}
    Since we have proved that \(i\) is differentiable then \(\DiffOperator i\) is differentiable which means \(i\) is twice differentiable and so on. Hence \(i \in C^{\infty}\).
\end{example}

As a matter of notation if \(\phi : V_1 \times \dots \times V_n\) be an \(n\)-linear then
\begin{equation*}
    \phi \cdot h_1\dots h_n := \func{\phi}{h_1, \dots , h_n}
\end{equation*}
particularly if \(V_1 = \dots = V_n\)
\begin{equation*}
    \phi \cdot h^n := \func{\phi}{h, \dots , h}
\end{equation*}
Now one can describe a homogeneous polynomial of degree \(k\) with a symmetric \(k\)-linear function
\begin{align*}
    \func{p}{x} = \phi \cdot x^k
\end{align*}
Then, \(\func{p}{x}\) is differentiable since
\begin{equation*}
    x \xmapsto[\calC^\infty]{\Delta} (x,\dots,x) \xmapsto[\calC^\infty]{\phi} p
\end{equation*}
and
\begin{align*}
    \operatorFunc{\DiffOperator \func{p}{x}}{h} & = \operatorFunc{\DiffOperator \func{\phi}{\func{\Delta}{x}} \circ \DiffOperator \func{\Delta}{x}}{h} \\
                                                & = \DiffOperator \phi \cdot x^n \circ \func{\Delta}{h}                                                \\
                                                & = k \phi \cdot x^{k-1}h                                                                              \\
    \implies \DiffOperator \func{p}{x}          & = k \phi \cdot x^{k-1}
\end{align*}

\begin{theorem}[Taylor approximation]
    Let \(f : U \to W\) be \(k\)-times differentiable at \(a\), then
    \begin{equation*}
        \func{p_k}{x} = \func{f}{a} + \diffOperator \func{f}{a} \cdot (x-a) + \dfrac{1}{2!} \diffOperator^2 \func{f}{a} \cdot (x-a)^2 + \dots + \dfrac{1}{k!} \diffOperator^k \func{f}{a} (x-a)^k
    \end{equation*}
    is \(k_\cardinalTH\) degree \textbf{Taylor} polynomial. Then the followings hold
    \begin{enumerate}
        \item
              \begin{equation*}
                  \lim_{x \to a} \dfrac{\func{f}{x} - \func{p_k}{x}}{\norm{x-a}^{k}} = 0
              \end{equation*}
        \item \(\func{p_k}{x}\) is the only \(k_\cardinalTH\) degree polynomial with such property.
        \item Additionally, if \(f\) is \((k + 1)\)-times differentiable in a neighbourhood of \(a\) then the remainder
              \begin{equation*}
                  \func{R}{x} = \func{f}{x} - \func{p_k}{x}
              \end{equation*}
              can be estimated with
              \begin{equation*}
                  \norm{\func{R}{b}} \leq \dfrac{1}{(k+1)!} \sup \set{\norm {\DiffOperator^{k+1} \func{f}{\xi}}} \norm{b-a}^{k+1}
              \end{equation*}
              where \(\xi\) is on line connecting \(a\) to \(b\).
    \end{enumerate}
\end{theorem}

\begin{proof} \leavevmode
    \begin{enumerate}
        \item for \(k = 1\) it is equivalent to differentiability of \(f\). By induction, assume it is true for \(k = n -1\) and let \(\func{g}{x} = \func{f}{x} - \func{p_k}{x}\) then  \footnote{Differentiablity of order \(k\) implies differentiability of order \(k-1\) in a neighbourhood. }
              \begin{align*}
                  \DiffOperator \func{g}{x}        & = \DiffOperator \func{f}{x} - \DiffOperator \func{p_k}{x}                                                                                                                                           \\
                                                   & = \diffOperator \func{f}{x} - \DiffOperator\left[\func{f}{a} + \diffOperator \func{f}{a} \cdot (x-a)  + \dots + \dfrac{1}{n!} \diffOperator^n \func{f}{a} (x-a)^n \right]                           \\
                                                   & = \diffOperator \func{f}{x} - \left[ \diffOperator \func{f}{a}  + \dfrac{1}{1!} \diffOperator^2 \func{f}{a} \cdot (x-a) + \dots + \dfrac{1}{(n-1)!} \diffOperator^n \func{f}{a} (x-a)^{n-1} \right] \\
                  \intertext{which is equivalent to the proposition at \(n-1\) for \(\diffOperator \func{f}{a}\) and hence there exists a \(\delta > 0 \) such that if \(\norm{x-a} < \delta\)}
                  \norm{\DiffOperator \func{g}{x}} & \leq \epsilon \norm{x-a}^{n-1}
              \end{align*}
              by the \Cref{th:MultivariableMVT} we have
              \begin{align*}
                  \norm{\func{g}{x}} & = \norm{\func{g}{x} - \func{g}{a}} \leq \norm{x-a} \sup \norm{\DiffOperator \func{g}{\xi}} \\
                                     & \leq \epsilon \norm{x-a} \norm{\xi - a}^{k-1}                                              \\
                                     & \leq \norm{x-a}^{k}
              \end{align*}
        \item If there were two such polynomial \(p_1, p_2\) then for \(q = p_1 - p_2\) we have that
              \begin{equation*}
                  \lim_{x \to a} \dfrac{\func{q}{x}}{\norm{x-a}^k} = 0
              \end{equation*}
              then one can show that \(\func{q}{x} \equiv 0\). %TODO: do the rest
        \item Define \(g: \clcl{0}{1} \to W\) as such
              \begin{equation*}
                  \func{g}{t} = \func{f}{a + t(b-a)}
              \end{equation*}
              therefore
              \begin{equation*}
                  \func{g^{(n)}}{t} = \diffOperator^k \func{f}{a + t(b-a)} \cdot (b-a)^k
              \end{equation*}
              For each component of \(g\) we apply the single variable Taylor's approximation
              \begin{equation*}
                  \func{g_i}{1} - \sum_{n = 0}^{k} \dfrac{\func{g^{(n)}_i}{0}}{n!} = \dfrac{\func{g^{(k+1)}_i}{\xi_i}}{(k+1)!}
              \end{equation*}
              or equivalently
              \begin{multline*}
                  \norm{\func{R}{b}} =\norm{ \func{f}{b} - \sum_{n = 0}^{k} \dfrac{\diffOperator^n \func{f}{a} \cdot (b-a)^n}{n!} } \\= \dfrac{1}{(k+1)!} \norm{ {\begin{bmatrix}
                                  \diffOperator^{k+1} \func{f_1}{a + \xi_1(b-a)} \cdot (b-a)^k & \dots & \diffOperator^{k+1} \func{f}{a + \xi_m(b-a)} \cdot (b-a)^k
                              \end{bmatrix}} } %TODO: needs clearification.
              \end{multline*}
              which was what was wanted.
    \end{enumerate}
\end{proof}

\begin{theorem}
    Let \(f : U \to \Reals\) and \(p\) is an extremum of the function then
    \begin{equation*}
        \forall h, \ \operatorFunc{\DiffOperator \func{f}{p}}{h} = 0
    \end{equation*}
\end{theorem}

\begin{proof}
    For all \(h\) define \(g_h : \opop{-\epsilon}{\epsilon} \to \Reals\)
    \begin{equation*}
        \func{g_h}{t} = \func{f}{p + th}
    \end{equation*}
    then \(\func{g'_h}{0} = 0\).
\end{proof}

%TODO: needs proof
\begin{theorem}
    Let \(f : U \to \Reals\) be of \(\calC^2\), \(p\) be a critical point of \(f\), and \(\DiffOperator^2 \func{f}{p}\) be positive definite. Then, \(p\) is a local minimum of \(f\). (If \(\DiffOperator^2 \func{f}{p}\) is negative definite then \(p\) is local maxima.)
\end{theorem}
Assuming the following lemma
\begin{lemma} \label{lm:ContinuityOfPositiveDefinite}
    If \(\DiffOperator^2 f\) is continuous and positive definite at point \(p\) then it is positive definite in a neighbourhood of \(p\).
\end{lemma}
\begin{proof}
    We wish to prove that there exists a \(\delta > 0\) for all unit vectors in \(V\), \(e\), \( 0 < t < \delta\)
    \begin{equation*}
        \func{f}{p} \leq \func{f}{p + te}
    \end{equation*}
    To do so, define \(g_e : \opop{0}{\delta} \to \Reals\)
    \begin{equation*}
        \func{g_e}{t} = \func{f}{p + te}
    \end{equation*}
    then by the Taylor's theorem
    \begin{equation*}
        \func{g_e}{t} = \func{g}{0}  + \func{g'}{0}t + \dfrac{\func{g''}{\xi}}{2!}t^2
    \end{equation*}
    where \(\xi \in \opop{0}{t}\). Equivalently
    \begin{align*}
        \func{f}{p + te} & = \func{f}{p} + \operatorFunc{\DiffOperator \func{f}{p}}{e} + \dfrac{\diffOperator^2 \func{f}{p + t\xi} \cdot e^2}{2}t^2 \\
                         & = \func{f}{p} +  \dfrac{\diffOperator^2 \func{f}{p + t\xi} \cdot e^2}{2}t^2
    \end{align*}
    Using the \Cref{lm:ContinuityOfPositiveDefinite} there exists a neighbourhood of \(p\) such that
    \begin{equation*}
        \diffOperator^2 \func{f}{p + t\xi} \cdot h^2 > 0
    \end{equation*}
    for all \(h\) in the neighbourhood. Therefore,
    \begin{equation*}
        \func{f}{p + te} > \func{f}{p}
    \end{equation*}
    which is what we wanted.
\end{proof}

\section{Smoothness Classes}

Let \(f \in \calC^r\) then one can define the norm
\begin{equation*}
    \norm{f}_r = \max \set{\sup_{x \in U}\norm{\func{f}{x}}, \dots , \sup_{x \in U}\norm{\DiffOperator^r \func{f}{x}}}
\end{equation*}
and let the set of all such \(f\) with \(\norm{f}_r < \infty\) be denoted as \(\func{\calC^r}{U,W}\).

%TODO: needs proof
\begin{theorem}
    Uniform convergence in \(\calC^r\) is equivalent to Cauchy.
\end{theorem}

\begin{proof}

\end{proof}

\begin{theorem}
    \( \func{\calC^r}{U,W}\) under \(\norm{\cdot}_r\) is a Banach space.
\end{theorem}


\begin{definition}[Local convergence]
    A functional sequence \(f_n\) is \textbf{locally convergent} if for each \(x \in U\)  there exists a open set \(x \in V \subset U\) such that \(\left. f_n \right|_V\) is uniformly convergent.
\end{definition}

\begin{theorem}
    Let \(V,W\) be normed finite dimensional spaces, \(U \subset V\) is open and connected, \(x_0 \in U\) and \(f_n : U \to W\) is a sequence of differentiable function that
    \begin{enumerate}
        \item \(\func{f_n}{x_0}\) is convergent.
        \item \(\DiffOperator f_n : U \to \func{\calL}{V,W}\) is locally convergent to some function \(g : U \to \func{\calL}{V,W}\)
    \end{enumerate}
    then the sequence \(f_n\) is locally convergent to \(f : U \to W\) and \(\DiffOperator f = g\). Furthermore, because of connectedness of \(U\) for each \(x \in U\), \(\func{f_n}{x}\) is convergent.
\end{theorem}

%TODO: do the proof
\begin{proof}
    take open ball \(W\) around \(x_0\) such that \(\DiffOperator f_n|_W\) is uniformly convergent. then prove the first statement.
    \begin{equation*}
        \norm{\func{f_m}{x} - \func{f_n}{x}} \leq \norm{\operatorFunc{f_m - f_n}{x} - \operatorFunc{f_m - f_n}{x_0}} + \norm{\func{f_m}{x_0} - \func{f_n}{x_0}}
    \end{equation*}
    apply MVT here and make the bounds smaller using (2). Then prove the differentiability with e/3. To prove (3) use open/close argument.
\end{proof}

\section{Inverse function theorem}
Consider a function \(f\), we wish to find all the solutions to the equation
\begin{equation*}
    \func{f}{x} = y_0
\end{equation*}
To do so, we can define another function \(F_{y_0}\) such that
\begin{equation*}
    \func{F_{y_0}}{x} = x - \func{f}{x} + y_0
\end{equation*}
then if \(x\) is a solution to the equation, it is a fixed point of \(F_{y_0}\).

\begin{theorem} [Banach fixed point] \label{th:BanachFixedPoint}
    Let \(\metricSpace{X}{d}\) be a complete metric space and \(f : X \to X\) is such that for some \(0 \leq \lambda < 1\)
    \begin{equation*}
        \forall x,y \in X, \ \func{d}{\func{f}{x},\func{f}{y}} \leq \lambda \func{d}{x,y}
    \end{equation*}
    Then for each \(x \in X\) the sequence \(\set{\func{f^n}{x}}\) is convergent to \(p \in X\) such that \(\func{f}{p} = p \).
\end{theorem}

\begin{proof}
    Let \(x_n = \func{f^n}{x}\) for \(n \geq 0\) then
    \begin{equation*}
        \func{d}{x_n,x_{n+1}} \leq \lambda \func{d}{x_{n-1},x_{n}} \leq \lambda^n \func{d}{x_0,x_1}
    \end{equation*}
    thereofore
    \begin{equation*}
        \func{d}{x_n,x_m} \leq \sum_{i = n}^{m-1} \lambda^i \func{d}{x_0,x_1} \leq \func{d}{x_0,x_1} \dfrac{\lambda^n}{1 - \lambda}
    \end{equation*}
    hence \(\set{\func{f^n}{x}}\) is Cauchy and it is convergent to a point \(p\). Lastly,
    \begin{align*}
        \func{d}{\func{f}{p},p} & \leq \func{d}{\func{f}{p},\func{f}{x_n}} + \func{d}{\func{f}{x_n},p} \\
                                & \leq \lambda \func{d}{p,x_n} + \func{d}{x_{n+1},p} < \epsilon
    \end{align*}
\end{proof}

\begin{theorem}[Inverse function theorem]
    Let \(V,W\) be finite dimensional normed vector space such that \(\dim V = \dim W\) and \(U \subset V\) is open. If \(f : U \to W\) is continuously differentiable and for some \(a \in U\), \(\DiffOperator \func{f}{a}\) is invertible. Then, there are open set \(S \subset V\) and \(T \subset W\)such that \(a \in S  \subset U\) and \(\func{f}{a} \in T\) such that \(f|_S\) is bijective and \((f|_S)^{-1} = g\) where \(g \in \calC^1\) and
    \begin{equation*}
        \DiffOperator \func{g}{\func{f}{x}} = \left(\DiffOperator \func{f}{x}\right)^{-1}
    \end{equation*}
\end{theorem}

\begin{proof}
    Let \(S\) be an open convex set around \(a\) such that for all \(x \in S\)
    \begin{equation*}
        \norm{\DiffOperator \func{f}{x} - \DiffOperator \func{f}{a}} < \dfrac{1}{2} \norm{\DiffOperator \func{f^{-1}}{a}}^{-1}
    \end{equation*}
    hence \(\DiffOperator \func{f}{x}\) is invertible. Let \(T = \func{f}{S}\) then we shall prove the following
    \begin{enumerate}
        \item \(f|_S\) is bijective.

              Let \(\psi : S \to V\) with
              \begin{align*}
                  \func{\psi_y}{x}                                & = x - \left(\DiffOperator \func{f}{a}\right)^{-1} (\func{f}{x} - y)                                                            \\
                  \implies \DiffOperator \func{\psi_y}{x }        & = \DSOne_V - \left(\DiffOperator \func{f}{a}\right)^{-1} \DiffOperator \func{f}{x}                                             \\
                                                                  & = \left(\DiffOperator \func{f}{a}\right)^{-1} \circ \left( \DiffOperator \func{f}{a} - \DiffOperator \func{f}{x} \right)       \\
                  \implies \norm{\DiffOperator \func{\psi_y}{x }} & \leq \norm{\left(\DiffOperator \func{f}{a}\right)^{-1}} \norm{\DiffOperator \func{f}{a} - \DiffOperator \func{f}{x}}           \\
                                                                  & < \dfrac{1}{2} [\left(\DiffOperator \func{f}{a}\right)^{-1}] [\left(\DiffOperator \func{f}{a}\right)^{-1}]^{-1} = \dfrac{1}{2}
              \end{align*}
              therefore by mean value theorem
              \begin{equation*}
                  \norm{\func{\psi_y}{x_1} - \func{\psi_y}{x_2}} \leq \dfrac{1}{2} \norm{x_1 - x_2}
              \end{equation*}
              which follows that \(\psi_y\) has at most one fixed point because
              \begin{equation*}
                  \norm{\func{\psi_y}{x_1} - \func{\psi_y}{x_2}} = \norm{x_1 - x_2} \leq  \dfrac{1}{2} \norm{x_1 - x_2}
              \end{equation*}
              is a contradiction, and for that fixed point
              \begin{equation*}
                  \func{\psi_y}{x} = x - \left(\DiffOperator \func{f}{a}\right)^{-1} (\func{f}{x} - y) = x \implies y = \func{f}{x}
              \end{equation*}
              which means \(f\) is injective. By the definition of \(T\), \(f\) is surjective as well.

        \item  \(T\) is open.

              We wish to prove that for each \(\func{f}{x_0} = y_0 \in T\) we wish to prove there exist a \( \sigma > 0\) such that \(\func{B_\sigma}{y_0}\) is contained in \(T\). In other words, \(\forall y \in \func{B_\sigma}{y_0}\)
              \begin{align*}
                  \exists x \in S, \; \func{f}{x} = y \iff \func{\psi_y}{x} = x
              \end{align*}
              To apply the contraction fixed point we must find complete metric space \(X\) such that \(\func{\psi_y}{X} = X\). Choose \(\rho\) as small as needed that \(\overline{\func{B_\rho}{x_0}} \subset S\), which makes a complete metric space. Let \(\sigma =  \dfrac{r \rho}{2}\) where \(r = \norm{\left(\DiffOperator \func{f}{a}\right)^{-1}}^{-1}\). Lastly, we show that for each \(y \in \overline{\func{B_\sigma}{y_0}}\), \(\func{\psi_y}{ \overline{\func{B_\rho}{x_0}} }=  \overline{\func{B_\rho}{x_0}}\). That is, \(x \in \overline{\func{B_\rho}{x_0}}\) implies that \( \func{\psi_y}{x} \in \overline{\func{B_\rho}{x_0}}\).
              \begin{align*}
                   & \norm{\func{\psi_y}{x}  - x_0} \leq \norm{\func{\psi_y}{x} - \func{\psi_y}{x_0}} + \norm{\func{\psi_y}{x_0} - x_0}                              \\
                   & \leq  \dfrac{1}{2} \norm{x - x_0} +  \norm{\left(\DiffOperator \func{f}{a}\right)^{-1}(y - y_0)}  \leq \dfrac{\rho}{2} + \dfrac{\rho}{2} = \rho
              \end{align*}
        \item \(g = (f|_S)^{-1} : T \to S\) is continuously differentiable. Writting the differentiability criteria
              \begin{equation*}
                  \norm{\func{g}{y + h} - \func{g}{y} - \operatorFunc{\DiffOperator \func{g}{y}}{h}} \leq \epsilon \norm{h}
              \end{equation*}
              Let \(y = \func{f}{x}\) and \(y + h = \func{f}{x + k}\) then \(h = \func{f}{x + k} - \func{f}{x}\) and note
              \begin{equation*}
                  \norm{\func{\psi_y}{x + k} - \func{\psi_y}{x}} = \norm{k - \operatorFunc{\left(\DiffOperator \func{f}{a}\right)^{-1}}{h}} \leq \dfrac{1}{2} \norm{k}
              \end{equation*}
              which implies
              \begin{equation*}
                  \dfrac{1}{2}\norm{k} \leq \norm{\operatorFunc{\left(\DiffOperator \func{f}{a}\right)^{-1}}{h}} \leq \dfrac{3}{2} \norm{k}
              \end{equation*}
              \begin{align*}
                  \norm{k - \operatorFunc{\left(\DiffOperator \func{f}{x}\right)^{-1}}{\func{f}{x + k} - \func{f}{x}}} & =                                                                                                             \norm{\operatorFunc{\left(\DiffOperator \func{f}{x}\right)^{-1}}{\operatorFunc{\DiffOperator \func{f}{x}}{k} - \func{f}{x + k} - \func{f}{x}}} \\
                                                                                                                       & \leq \norm{\left(\DiffOperator \func{f}{x}\right)^{-1}} \norm{\func{f}{x + k} - \func{f}{x} - \operatorFunc{\DiffOperator \func{f}{x}}{k}}                                                                                                                   \\
                                                                                                                       & \leq \norm{\left(\DiffOperator \func{f}{x}\right)^{-1}} \epsilon \norm{k}                                                                                                                                                                                    \\
                                                                                                                       & \leq 2 \norm{\left(\DiffOperator \func{f}{x}\right)^{-1}} \norm{\left(\DiffOperator \func{f}{a}\right)^{-1}} \epsilon
              \end{align*}
              which proves the differentiability of \(g\) as \(\norm{\left(\DiffOperator \func{f}{x}\right)^{-1}}\) is bounded in \(S\). As shown, the inverse operator is \(i\) is continuous and therefore if \(\DiffOperator f\) is continuous, then \(\func{i}{\DiffOperator f}\) is continuous. In fact, if \(f \in C^k\) then \(g \in C^k\) as well.
    \end{enumerate}
\end{proof}

\begin{corollary}
    If \(f\) is continuously differentiable and \(\DiffOperator \func{f}{x}\) is invertible for every \(x \in U\), then for any open set \(S\), \(\func{f}{S}\) is an open set as well.
\end{corollary}

\begin{proof}
    By the inverse function theorem for each \(x \in S\) there is an open set \(U_x\) in \(S\) and \(V_x\) in \(W\) such that \(\func{f}{U_x} = V_x\), therefore
    \begin{equation*}
        \func{f}{S} =  \func{f}{\bigcup U_x} = \bigcup \func{f}{U_x} = \bigcup V_x
    \end{equation*}
    which is an open set.
\end{proof}

\section{Implicit function}
\begin{theorem}
    Let \(V,W\) be finite dimensional normed vector spaces and \(U \subset V \times W\) is open. If \(f: U \to W\), \(f \in \calC^1\) where \(\func{f}{x_0,y_0} = z_0\) and \(\operatorFunc{\DiffOperator f|_{\set{x_0} \times W}}{x_0,y_0}\) is invertible then there exist open set \(S\) around \(x_0\) and \(T\) around \(y_0\) that \(S \times T \subset U\), such that for each \(x \in S\) there exists a unique \(y \in T\) with
    \begin{equation*}
        \func{f}{x,y} = z_0
    \end{equation*}
    hence there is a continuously differentiable function \(\phi: S \to T\) such that \(\func{\phi}{x} = y\) where \(\func{f}{x,y} = z_0\) and
    \begin{equation*}
        \DiffOperator \phi = - \left(\DiffOperator_y f\right)^{-1} \DiffOperator_x f
    \end{equation*}
\end{theorem}

\begin{proof}
    To apply the inverse function theorem, we need a function whose domain and range have the same dimension. So define, \(F: U \to V \times W\)
    \begin{equation*}
        \func{F}{x,y} = (x,\func{f}{x,y})
    \end{equation*}
    Then
    \begin{equation*}
        \DiffOperator \func{F}{x_0,y_0} = \left[\begin{array}{c|c}
                I_n                                                           & \DSZero                                                       \\ \hline
                \operatorFunc{\DiffOperator f|_{\set{y_0} \times U}}{x_0,y_0} & \operatorFunc{\DiffOperator f|_{\set{x_0} \times W}}{x_0,y_0}
            \end{array}\right]
    \end{equation*}
    Since \(I_n\) and \(\operatorFunc{\DiffOperator f|_{\set{x_0} \times W}}{x_0,y_0}\) are both invertible then \(\DiffOperator \func{F}{x_0,y_0}\) is invertible as well. By inverse function theorem there are open set \(\Omega_1\) around \((x_0,y_0)\) and \(\Omega_2\) around \((x_0,z_0)\) such that \(F|_{\Omega_1}\) is \(\calC^1\) diffeomorphism from \(\Omega_1\) to \(\Omega_2\). Let \(S = \set[x]{(x,z_0) \in \Omega_2}\) and the set \(T = \set[y]{(x,y) \in \Omega_1}\) , then we shall prove that for each \(x \in S\) there exists exactly one \(y \in T\) \footnote{show that they are open}. If \(x \in S\) then there exists \((x,y) \in \Omega_1\) such that \(\func{F}{x,y} = (x,z_0)\), suppose that there are two such \(y\), \(y_1\) and \(y_2\) for \(x\)
    \begin{equation*}
        \func{F}{x,y_1} = (x,\func{f}{x,y_1}) = (x,z_0) = (x,\func{f}{x,y_2}) = \func{F}{x,y_2}
    \end{equation*}
    which since \(F|_{\Omega_1}\) is injective then \(y_1 = y_2\).

    Let \(G: \Omega_2 \to \Omega_1\) be the local inverse of \(F\) and \(\phi : S \to T\)
    \begin{equation*}
        \func{\phi}{x} = \operatorFunc{ \pi_2 \circ G}{x,z_0}
    \end{equation*}
    which implies that \(\phi\) is continuously differentiable. Lastly,
    \begin{align*}
        \DiffOperator \func{f}{x,\func{\phi}{x}} & = \DiffOperator \func{f}{x,\func{\phi}{x}} \circ (I, \DiffOperator \func{\phi}{x})                                         \\
                                                 & = \DiffOperator_x \func{f}{x,\func{\phi}{x}} + \DiffOperator_y  \func{f}{x,\func{\phi}{x}}\DiffOperator \func{\phi}{x} = 0 \\
        \implies  \DiffOperator \phi             & = - \left(\DiffOperator_y f\right)^{-1} \DiffOperator_x f
    \end{align*}
\end{proof}


\section{Rank theorem}
A generalization of \(PAQ = \begin{bmatrix}
    I_r & 0 \\
    0   & 0 \\
\end{bmatrix}\)
\begin{theorem}
    Let \(f: U \to W\) be of class \(\calC^1\) and
    \begin{equation*}
        \forall x \in U, \; \rank \DiffOperator \func{f}{x} = k
    \end{equation*}
    then for each \(p \in U\) there exist open subsets \(p \in U_0\) and \(\func{f}{p} \in W_0\) and diffeomorphisms
    \begin{align*}
        \alpha & : U_0 \to U'_0 \\
        \beta  & : V_0 \to V'_0
    \end{align*}
    such that
    \begin{equation*}
        \func{ \beta \circ f \circ a^{-1} }{x_1, \dots, x_n} = (x_1, \dots , x_k, 0 , \dots, 0)
    \end{equation*}
\end{theorem}

\begin{proof}
    Without loss of generality, assume \(V = \Reals^n\), \(W = \Reals^m\), with a transition \(p = 0 \in \Reals^n\), \(\func{f}{p} = 0 \in \Reals^m\), and with a change of basis,
    \begin{equation*}
        \DiffOperator \func{f}{p} = \begin{bmatrix}
            I_k     & \DSZero \\
            \DSZero & \DSZero \\
        \end{bmatrix}
    \end{equation*}
    Suppose
    \begin{equation*}
        \func{f}{x,y} = (\func{f_1}{x,y}, \func{f_2}{x,y})
    \end{equation*}
    where \(x \in \Reals^k\), \(y \in \Reals^{n-k}\), \(\func{f_1}{x,y} \in \Reals^k\), and \(\func{f_2}{x,y} \in \Reals^{m-k}\). Then
    \begin{equation*}
        \DiffOperator \func{f}{0,0} = \begin{bmatrix}
            I_k     & \DSZero \\
            \DSZero & \DSZero
        \end{bmatrix}
    \end{equation*}

    Let \(\func{H}{x,y,u} := u - \func{f_1}{x,y}\), where \(u \in \Reals^k\). Note that \(H \in \calC^1\) and \(\DiffOperator_x H = -\DiffOperator_x f_1\) is invertible at \((0,0,0)\). Therefore, there are open set \(\Omega_1\) around \((0,0) \in \Reals^{k} \times \Reals^{n-k}\) and \(\Omega_2\) around \(0 \in \Reals^k\) such that \(H|_{\Omega_1}\) is bijective and there exists a function \(\phi: \Omega_1 \to \Omega_2\) such that
    \begin{equation*}
        x = \func{\phi}{u,y}
    \end{equation*}
    whenever \(\func{H}{x,y,u} = 0\). Then let \(\alpha^{-1} : \Omega_1 \to \Omega_2 \times \Reals^{n-k}\) be
    \begin{equation*}
        \func{\alpha^{-1}}{u,y} = (\func{\phi}{u,y},y)
    \end{equation*}
    which is a diffeomorphism around \((0,0) \in \Reals^{k} \times \Reals^{n-k}\) and
    \begin{equation*}
        \func{f \circ \alpha^{-1}}{u,y} = (\func{f_1}{\func{\phi}{u,y},y}, \func{f_2}{\func{\phi}{u,y},y}) = (u, \func{\eta}{u,y})
    \end{equation*}
    \(\DiffOperator_y \eta \equiv 0\) that is \(\func{\eta}{u,y} = \func{\eta}{u,0}\) since, \(\func{\rank}{\DiffOperator \func{f \circ \alpha^{-1}}{u,y}} = \func{\rank}{\DiffOperator \func{f}{u,y}}\) and thus
    \begin{equation*}
        \DiffOperator \func{f \circ \alpha^{-1}}{u,y} = \begin{bmatrix}
            I_k                  & \DSZero              \\
            \DiffOperator_u \eta & \DiffOperator_y \eta
        \end{bmatrix}
    \end{equation*}
    Let \(\beta : \Reals^k \times \Reals^{m-k} \to \Reals^k \times \Reals^{m-k}\)
    \begin{equation*}
        \func{\beta}{u,z} = (u,z - \func{\eta}{u,z})
    \end{equation*}
    which is clear diffeomorphism around \((0,0) \in \Reals^k \times \Reals^{m-k}\) which means that
    \begin{equation*}
        \func{ \beta \circ f \circ \alpha^{-1} }{x,y} = (x,0)
    \end{equation*}
\end{proof}

\section{Lagrange Multiplier}
\begin{theorem}
    Let \(f: U \subset \Reals^n \to \Reals\) is differentiable and \(g : U \to \Reals\) is continuously differentiable, \(S = \func{g^{-1}}{0} \subset U\) and \(\nabla \func{g}{s} \neq 0\), \(\forall s \in S\). Assume \(x_0 \in S\) 
    \begin{equation*}
        \max_{x \in S} \func{f}{x} = \func{f}{x_0}
    \end{equation*}
    then there exists \(\lambda \in \Reals\) such that 
    \begin{equation*}
        \nabla f|_{x_0} = \lambda \nabla g|_{x_0}
    \end{equation*}
\end{theorem}

\begin{proof}
    Since \(\rank \DiffOperator g = 1\) everywhere, then there are diffeomorphism \(\alpha,\beta\) such that 
    \begin{equation*}
        \func{\beta \circ g \circ \alpha^{-1}}{t_1, \dots ,t_n} = t_n
    \end{equation*}
\end{proof}

{\Large\textbf{Exercises}}
\begin{enumerate}
    \item Using l'Hopital's rule show that
          \begin{equation*}
              \lim_{t \to 0} \dfrac{\func{\Delta}{t,h,k}}{t^2} = \dfrac{\operatorFunc{\diffOperator \func{f}{a}}{h,k} + \operatorFunc{\diffOperator \func{f}{a}}{k,h}}{2}
          \end{equation*}
\end{enumerate}
\newpage
