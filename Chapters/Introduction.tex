\chapter{Introduction to Lattice}
\begin{definition}
    Let \(b_1, \dots, b_n \in \Reals^m\) be \(n\) linearly independent vectors. The \textbf{lattice} generated by these vectors is denoted as \(\func{\calL}{b_1, \dots, b_n}\) and 
    \begin{equation*}
        \func{\calL}{b_1, \dots, b_n} = \set< \sum_{i = 1}^n x_i b_i>{x_i \in \Integers}
    \end{equation*}
    If we let \(B = \begin{bmatrix}
        b_1 & b_2 & \dots & b_n
    \end{bmatrix}\), then  
    \begin{equation*}
        \func{\calL}{B} = \set<Bx>{x \in \Integers^n}
    \end{equation*}
    If \(n = m\), then the lattice is said to be \textbf{full rank}. \(m\) is the dimension and \(n\) is the rank of the lattice.
\end{definition}

-- the case where \(\func{\calL}{B}\) is not a lattice.
\section{Description of lattices}
\subsection{Algebraic description}
\begin{definition}
    A matrix \(U \in \Integers^{n \times n}\) is \textbf{unimodular} if \(\abs{\det U} = 1\). 
\end{definition}
\begin{proposition}
    The unimodular matrices form a group under matrix multiplication.
\end{proposition}
\begin{proof}
    Clearly, \(I\) is a unimodular matrix and is the identity element of the group. By definition, a unimodular matrix \(U\) is invertible and \(\abs{\det U^{-1}} = 1\). Also, note that 
    \begin{equation*}
        U^{-1} = \frac{1}{\det U} \func{\adjoint}{U}
    \end{equation*}
    where the adjugate matrix \(\func{\adjoint}{U}\) is an integer matrix. Thus, \(U^{-1} \in \Integers^{n}\). The associativity follows from the associativity of matrix multiplication.
\end{proof}

\begin{theorem}\label{thm:BasisEquivalencyUptoUnimodular}
    Two full rank matrix \(B,B' \in \Reals^n\) produce the same lattice if and only if there exists a unimodular matrix \(U\) such that \(B' = BU\).
\end{theorem}

\subsection{Geometric description}
\begin{definition}
    Suppose \(b_1, \dots, b_n \in \Reals^m\) are linearly independent. The \textbf{fundamental parallelopiped} of these vectors is 
    \begin{equation*}
        \func{\calP}{b_1,\dots, b_n} = \set<\sum_{i=1}^n x_i b_i>{x_i \in \clop{0}{1}}
    \end{equation*}
\end{definition}

\begin{theorem}
    Suppose \(\Lambda\) is a full rank \(n\)-dimensional lattice and \(b_1, \dots, b_n \in \Reals^n\) are linearly independent vectors in \(\Lambda\). Then \(b_1, \dots, b_n\) are a basis for \(\Lambda\) if and only if 
    \begin{equation*}
        \Lambda \cap \func{\calP}{b_1,\dots, b_n} = \set{0}
    \end{equation*}
\end{theorem}

\section{Determinant of lattice}
\begin{definition}
    Let \(\Lambda\) be a lattice generated basis \(B\). The \textbf{determinant} of \(\Lambda\) is the volume of fundamental parallelopiped of \(B\).
    \begin{equation*}
        \det \Lambda = \func{\vol}{\func{\calP}{B}}
    \end{equation*}
    It can be shown that \(\func{\vol}{\func{\calP}{B}} = \sqrt{\det B^T B}\). To show that this definition is well-defined, we must prove that for any basis two \(B, B'\), the volumes of fundamental parallelopipeds are equal. Since, \(B\) and \(B'\) generate the same lattice, by \ref{thm:BasisEquivalencyUptoUnimodular}, there exists a unimodular matrix \(U\) such that \(B' = BU\).
    \begin{align*}
        \func{\vol}{\func{\calP}{B'}} &= \sqrt{\det B^{'T}B'}\\
        &= \sqrt{\det (BU)^T BU}\\
        &= \sqrt{\det U^T B^T B U}\\
        &= \sqrt{\det U^T \det B^T B \det U}\\
        &= \sqrt{(\det U)^2 \det B^T B}\\
        &= \sqrt{\det B^T B} = \func{\vol}{\func{\calP}{B}}
    \end{align*}
    which was what was wanted. 
\end{definition}

Intuitively, the \(\det \Lambda\) is inversely proportional to its density. 

\begin{remark}
    In mathematical analysis, the volume -- or length or area -- of a set is measured with \textit{measures}. The exact definition of a measure is beyond the scope this text, however, we will almost always use the \textit{lebesgue measure}, unless stated otherwise. Measures can be defined on any set, and hence the measure of set may not depend on a particular metric. As a result, we are able to consider the same space with the same measure under different metrics or norms without affecting the measure. 
\end{remark}
\section{Gram-Schmidt}
In Gram-Schmidt procedure, a set of linearly independent vectors \(b_1, \dots, b_n\) are transformed into a set of orthogonal vectors \(b_1^{\ast}, \dots, b_n^{\ast}\).
\begin{equation*}
    b_i^{\ast} = b_i - \sum_{j = 1}^{i - 1} \dfrac{\angleBracket{b_i,b_j^{\ast}}}{\angleBracket{b_j^{\ast}, b_j^{\ast}}} b_j^{\ast} = b_i - \sum_{j = 1}^{i - 1} u_{i,j} b_j^{\ast}
\end{equation*}
with \(b_1^{\ast} = b_1\).
\begin{proposition}
    \ 
    \begin{enumerate}
        \item For all \(i \neq j\), \(\angleBracket{b_i^{\ast},b_j^{\ast}} = 0\).
        \item For all \(i > j\), \(\angleBracket{b_i^{\ast}, b_j} = 0\).
        \item For all \(i\), \(\vspan \set{b_1, \dots, b_i} = \vspan \set{b_1^{\ast}, \dots, b_i^{\ast}}\).
        \item If \(B = \begin{bmatrix}
            b_1 & \dots & b_n
        \end{bmatrix}\) and \(B^{\ast} = \begin{bmatrix}
            b_1^{\ast} & \dots & b_n^{\ast}
        \end{bmatrix}\), then 
        \begin{equation*}
            B = B^{\ast} \begin{bmatrix}
                1 & u_{2,1} & \dots & u_{n,1}\\
                0 & 1 & \dots & u_{n,2}\\
                \vdots& \vdots& \ddots& \vdots \\
                0 & 0 & \dots & 1
            \end{bmatrix}
        \end{equation*}
    \end{enumerate}
\end{proposition}
\begin{lemma}
    If we apply the Gram-Schmidt procedure to \(B \in \Reals^{m \times n}\) and get \(B^{\ast} \in \Reals^{m \times n}\), then 
    \begin{equation*}
        \det B^T B = \prod_{i = 1}^n \norm{b_i^{\ast}}^2
    \end{equation*}
\end{lemma}
\begin{proof}
    Note that,
    \begin{equation*}
        B^{\ast} \begin{bmatrix}
            1 & u_{2,1} & \dots & u_{n,1}\\
            0 & 1 & \dots & u_{n,2}\\
            \vdots& \vdots& \ddots& \vdots \\
            0 & 0 & \dots & 1 
        \end{bmatrix} = \begin{bmatrix}
            \dfrac{b_1^{\ast}}{\norm{b_1^{\ast}}} & \dots & \dfrac{{b_1^{\ast}}}{\norm{b_n^{\ast}}}
        \end{bmatrix}
            \begin{bmatrix}
            \norm{b_1^{\ast}} & u_{2,1}\norm{b_1^{\ast}} & \dots & u_{n,1}\norm{b_1^{\ast}}\\
            0 & \norm{b_2^{\ast}} & \dots & u_{n,2}\norm{b_2^{\ast}}\\
            \vdots& \vdots& \ddots& \vdots \\
            0 & 0 & \dots & \norm{b_n^{\ast}}
        \end{bmatrix}
    \end{equation*}
    Let \(B^{\ast'}\) be the orthonormal Gram-Schmidt matrix as calculated above and \(U'\) its corresponding upper triangular matrix.
    \begin{align*}
        \det B^T B &= \func{\det}{ (B^{\ast}U)^T B^{\ast} U}\\
        &= \func{\det}{(U')^T (B^{\ast'})^T B^{\ast'} U'}\\
        &= \det U' \det (B^{\ast'})^T B^{\ast'} \det U'\\
        &= \prod_{i = 1}^n \norm{b_i^{\ast}}^2 \det (B^{\ast'})^T B^{\ast'}
    \end{align*}
    Behold, the columns of \(B^{\ast'}\) are orthonormal therefore, \((B^{\ast'})^T B^{\ast'} = I_n\) and hence 
    \begin{equation*}
        \det B^T B = \prod_{i = 1}^n \norm{b_i^{\ast}}^2
    \end{equation*}
    which was what was wanted. 
\end{proof}

\section{Successive Minima}
Let \(\func{\lambda_i}{\Lambda}\) be the minimum norm of the longest vector among any set \(i\) linearly independent vectors in \(\Lambda\). 
\begin{equation*}
    \func{\lambda_i}{\Lambda} = \min_{\substack{\set{y_1, \dots, y_i} \\ \mathrm{lin \ indp}}} \max_{1 \leq j \leq i} \norm{y_j}
\end{equation*}
or equivalently 
\begin{equation*}
    \func{\lambda_i}{\Lambda} = \inf \set<r>{\dim \func{\vspan}{ \Lambda \cap \func{B_r}{0} }\geq i}
\end{equation*}
\begin{theorem}
    Let \(\Lambda\) be a littice of rank \(n\) with successive minima \(\func{\lambda_1}{\Lambda}, \dots, \func{\lambda_n}{\Lambda}\). There exists a set of linearly independent vectors \(v_1, \dots, v_n \in \Lambda\) such that \(\norm{v_i} = \func{\lambda_i}{\Lambda}\). 
\end{theorem}
\subsection{Lower bound on \(\lambda_1\)}
\begin{theorem}
    Let \(\func{\calL}{B}\) be a lattice, then
    \begin{equation*}
        \func{\lambda_1}{\func{\calL}{B}} \geq \min_j \norm{b_j^{\ast}}
    \end{equation*}
    and more generally 
    \begin{equation*}
        \func{\lambda_i}{\func{\calL}{B}} \geq \min){j \geq i} \norm{b_j^{\ast}}
    \end{equation*}
\end{theorem}
\begin{proof}
    Let \(x \in \Integers^n\), we will show that \(\norm{Bx} \geq \min_j \norm{b_j^{\ast}}\) for all \(x \in \Integers^n\). Note that, for any \(i\) we have
    \begin{equation*}
        \abs{\angleBracket{Bx, b_i^{\ast}}} = \abs{\sum_{j = 1}^n x_j\angleBracket{b_j , b_i^{\ast}}} = \abs{\sum_{j = i}^n x_j\angleBracket{b_j , b_i^{\ast}}} 
    \end{equation*}
    Let \(i\) be the largest indext that \(x_i \neq 0\). That is, for all \(j > i\), \(x_j = 0\). Thus 
    \begin{equation*}
        \abs{\angleBracket{Bx, b_i^{\ast}}} = \abs{x_i\angleBracket{b_i , b_i^{\ast}}} = \abs{x_i} \norm{b_i^{\ast}}^2 \leq \norm{b_i^{\ast}}^2
    \end{equation*}
    Moreover, by Cauchy-Schwarz inequality 
    \begin{equation*}
        \abs{\angleBracket{Bx, b_i^{\ast}}} \leq \norm{Bx} \norm{b_i^{\ast}}
    \end{equation*}
    ans hence 
    \begin{equation*}
        \norm{Bx} \geq \norm{b_i^{\ast}} \geq \min_j \norm{b_j^{\ast}}
    \end{equation*}
    which was what was wanted.
\end{proof}

\begin{corollary}
    For all lattices \(\Lambda\), there exists a constant \(\func{\epsilon}{\Lambda} > 0\) such that for all \(x,y \in \Lambda\) we have 
    \begin{equation*}
        \norm{x - y} \geq \func{\epsilon}{\Lambda}
    \end{equation*}
\end{corollary}
\begin{proof}
    Note that \(x -y \in \Lambda\) then, let \(\func{\epsilon}{\Lambda} = \func{\lambda_1}{\Lambda}\).
\end{proof}

\begin{theorem}
    A set \(\Lambda \subset \Reals^m\) is a lattice if and only if it is a discrete additive subgroup of \(\Reals^m\).
\end{theorem}
\section{Minkowski's Theorems}
\begin{theorem}[Blichfeld theorem]
    For any \(\Lambda\) and for any measurable set \(S \subset \vspan \Lambda\), if \(S\) has a volume \(\func{\vol}{S} > \det \Lambda\), then there exists two distinct points \(z_1,z_2 \in S\) such that \(z_1 - z_2 \in \Lambda\).    
\end{theorem}

\begin{theorem}[Convex body theorem]
    For any lattice \(\Lambda\) of rank \(n\) and any convext set \(S \subset \vspan \Lambda\) symmetric about the origin, if \(\func{\vol}{S} > 2^n \det\Lambda\), then \(S\) contains a non-zero lattice point.    
\end{theorem}

\begin{theorem}[Minkowski's first theorem]
    For any lattice \(\Lambda\),
    \begin{equation*}
        \func{\lambda_1}{\Lambda} \leq \sqrt{n} \bracket{\det \Lambda}^{\frac{1}{n}}
    \end{equation*}
\end{theorem}

\begin{theorem}[Minkowski's second theorem]
    For any lattice \(\Lambda\) of rank \(n\) under the \(l_2\) norm
    \begin{equation*}
        \bracket{\prod_{i=1}^n\func{\lambda_i}{\Lambda}}^{\frac{1}{n}} \leq \sqrt{n} \bracket{\det \Lambda}^{\frac{1}{n}}
    \end{equation*}
\end{theorem}
tightness of Minkowski's upper bounds.


\section{Dual lattice}
\begin{definition}
    The \textbf{dual lattice} or \textbf{reciprocal lattice} of \(\Lambda\), denoted by \(\Lambda^{\ast}\) is defined as 
    \begin{equation*}
        \Lambda^{\ast} = \set<x \in \vspan \Lambda>{\forall y \in \Lambda, \angleBracket{x,y} \in \Integers}
    \end{equation*}
    we can find \(U = \begin{bmatrix}
        u_1 & \dots & u_n
    \end{bmatrix}\)
    such that \(Uv_i = e_i\) by setting \(U = V(V^TV)^{-1}\). If \(\Lambda^{\ast} = \Lambda\), the lattice is called \textbf{self-dual}.
\end{definition}

\begin{proposition}\ 
    \begin{enumerate}
        \item \(\bracket{k \Integers^n}^{\ast} = \frac{1}{k} \Integers^n\).
        \item \(\bracket{\Lambda^{\ast}}^{\ast} = \Lambda\).
        \item \(\Lambda^{\ast}\) is a lattice and has rank \(n\).
        \item If \(B\) is a basis for \(\Lambda\), then there exists a unique \(D\) corresponding to \(B\) such that \(D\) is a basis for \(\Lambda^{\ast}\) and 
        \begin{enumerate}
            \item \(\vspan D = \vspan B\).
            \item \(B^T D = I\).
        \end{enumerate}
        \item \(\det \Lambda^{\ast} = \frac{1}{\det \Lambda}\).
    \end{enumerate}
\end{proposition}

\section{Computational problems}
\begin{definition}[Shortest vector problem]
    Given a basis \(B \in \Integers^{m \times n}\) find a non-zero lattice vector \(Bx\) such that \(\norm{Bx} \leq \norm{By}\) for any other vector \(y \in \Integers^{n} \backslash \set{0}\)
\end{definition}
\section{Complexity theory}
A Turing machine rnus in time \(\func{t}{n}\) if for all string \(w\) of size \(\abs{w} = n\), the turing machine halts in at most \(\func{t}{n}\) steps. If \(\func{t}{n} = a + n^b\) for some constants \(a,b\), we say that the turing machine runs in \textbf{polynomial time}. The class of decision problems that can be solved by a deterministic turing machine in polynomial time is denoted by \(\compClass{P}\). The class of decision problems that can be solved by a non-deterministic turing machine in polynomial time is denoted by \(\compClass{NP}\). The \(\compClass{NP}\) class can also be characterized by the class of languages \(L\) such that there exists a relation \(R \subset \Sigma^{\ast} \times \Sigma^{\ast}\) such that \((x,y) \in R\) can be checked in polynomial time in \(\abs{x}\) and \(x \in L\) if and only if there exists a \(y\) that \((x,y) \in R\). Then, \(y\) is called the \(\compClass{NP}\)-witness of \(x\).


The language \(A\) reduces to \(B\) if there exists a polynomial time computable function \(f:\Sigma^{\ast} \to \Sigma^{\ast}\) such that \(x \in A\) if and only if \(\func{f}{x} \in B\), denoted by \(A \mapsto B\), and it is called the \textbf{Karp reduction}. \(A\) is \(\compClass{NP-hard}\) if for all \(B \in \compClass{NP}\), \(B \mapsto A\). \(A\) is \(\compClass{NP-complete}\) if \(A\) is \(\compClass{NP-hard}\) and \(A \in \compClass{NP}\). 

Similarly, for \textbf{Cook reduction}, the language \(A\) reduces to \(B\) if there exists a polynomial time turing machine with access to an oracle that solves \(B\) that solves \(A\).

\section{Some lattice problems}
\begin{definition}[Closest vector problem]
    Given \(B \in \Integers^{m \times n}\) and a target vector \(t \in \Integers^{m}\) find \(Bx \in \Integers^m\) such that \(\norm{Bx - t} \leq \norm{By - t}\) for all \(y \in \Integers^n \backslash \set{0}\). There other variants to this problem.
    \begin{description}
        \item[Search] find \(Bx \in \Integers^m\) such that \(\norm{Bx - t}\) is minimized.
        \item[Optimization] Find the minimum of \(\norm{Bx -t}\).
        \item[Decision] Given a rational number \(r > 0\), decide if there exists \(x\) with \(\norm{x - t} < r\). 
    \end{description}
    Note that the decision problem reduces to optimization problem which itself reduces to search problem.
\end{definition}
-- the reation of \(\lambda_i\) to each other.
\begin{definition}[Approximate \(\compProblem{SVP}\)]
    Given a constant \(\gamma\), find a non-zero vector \(Bx\) such that \(\norm{Bx} \leq \gamma\norm{By}\) for all \(y \in \Integers^{n} \backslash \set{0}\).
\end{definition}
Approximate \(\compProblem{CVP}\) is defined similarly.

A list of polynomial time lattice problmes.
\begin{enumerate}
    \item Membership: Given \(B\) and \(x\), decide whether \(x \in \func{\calL}{B}\).
    \item Kernel: Given \(A \in \Integers^{m \times n}\) find the a basis for \(\Lambda = \set<x \in \Integers^n>{Ax = 0}\).
    \item  Kernel-mod: Given \(A \in \Integers_M^{m \times n}\) find the a basis for \(\Lambda = \set<x \in \Integers^n>{Ax = 0 \mod M}\).
    \item Basis: Given vectors \(b_1, \dots , b_n\) find a basis for the lattice generated by \(b_1, \dots ,b_n\). It is done by \textit{normal Hermitian form}, \(H\). \(H\) is the worst basis.
    \item Union: Given bases \(B_1 , B_2 \in \Integers^{m \times n}\), find a basis for \(\func{\calL}{B_1} \cup \func{\calL}{B_2}\).
    \item Dual: Find a basis for the dual lattice.
    \item Intersection: Given bases \(B_1 , B_2 \in \Integers^{m \times n}\), find a basis for \(\func{\calL}{B_1} \cap \func{\calL}{B_2}\).
    \item Equivalence: Given bases \(B_1 , B_2 \in \Integers^{m \times n}\), determine whether  \(\func{\calL}{B_1} = \func{\calL}{B_2}\).
    \item Cyclic: Determine whether the lattice \(\Lambda\) is cyclic. The lattice \(\Lambda\) is cyclic if for all \(x \in \Lambda\), all cyclic permutations of coordinates of \(x\) are in \(\Lambda\) as well.
\end{enumerate}
\section{Hardness of approximation}
\begin{definition}
    The promise is a pair \((\Pi_{yes}, \Pi_{no})\) with \(\Pi_{yes},\Pi_{no} \subset \Sigma^{\ast}\) and \(\Pi_{yes} \cap \Pi_{no} = \emptyset\).
\end{definition}

\begin{definition}
    An algorithm or turing machine solves a promise \((\Pi_{yes}, \Pi_{no})\) if for all \(w \in \Pi_{yes} \cup \Pi_{no}\), it can determine whether \(w \in \Pi_{yes}\) or \(w \in \Pi_{no}\).
\end{definition}

\begin{definition}
    The \(\compProblem{GAPSVP}_{\gamma}\) is a promise defined as follows:
    \begin{align*}
        \Pi_{yes} &= \set<(B,r)>{B \text{ is a basis}, B \in \Integers^{m \times n}, r \in \Rationals, \text{ and there exists } z \in \Integers^n \backslash \set{0}\ \suchThat \ \norm{Bz} < r}\\
        \Pi_{no} &= \set<(B,r)>{B \text{ is a basis}, B \in \Integers^{m \times n}, r \in \Rationals, \text{ and for all } z \in \Integers^n \backslash \set{0}  \ \suchThat \ \norm{Bz} > \gamma r}
    \end{align*}
    The \(\compProblem{GAPCVP}_{\gamma}\) is a promise defined as follows:
    \begin{align*}
        \Pi_{yes} &= \set<(B,t,r)>{B \text{ is a basis}, B \in \Integers^{m \times n}, t \in \Integers^m, r \in \Rationals, \exists z \in \Integers^n \backslash \set{0},\norm{Bz - t} < r}\\
        \Pi_{no} &= \set<(B,t,r)>{B \text{ is a basis}, B \in \Integers^{m \times n}, t \in  \Integers^m, r \in \Rationals, \forall z \in \Integers^n \backslash \set{0}, \norm{Bz - t} > \gamma r}
    \end{align*}
\end{definition}
\begin{theorem}
    \(\compProblem{GAPSVP}_{\gamma} \mapsto \compProblem{APPROXSVP}_{\gamma}\).
    \(\compProblem{APPROXSVP}_{\gamma} \mapsto \compProblem{GAPSVP}_{\gamma}\) and 
\end{theorem}

\begin{definition}
    A promise \((\Pi_{yes},\Pi_{no})\) is in \(\compClass{NP}\) when there exists a relation \(R \subset \Sigma^{\ast} \times \Sigma^{\ast}\) such that for all \(x \in \Pi_{yes}\) there exists \(y\) such that \((x,y) \in R\) and for all \(x \in \Pi_{no}\) for all \(y\), \((x,y) \notin R\).
\end{definition}

\begin{definition}
    Suppose \(f: \Sigma^{\ast} \to \Sigma^{\ast}\) is computable in polynomial time. A reduction from \((\Pi_{yes}, \Pi_{no})\) to \((\Pi'_{yes}, \Pi'_{no})\) when 
    \begin{equation*}
        \func{f}{\Pi_{yes}} \subset \Pi'_{yes} \ \mathrm{and} \ \func{f}{\Pi_{no}} \subset \Pi'_{no}
    \end{equation*}
\end{definition}
\begin{definition}
    \(\compClass{NP-hard}, \compClass{NP-complete}\) for promises.
\end{definition}